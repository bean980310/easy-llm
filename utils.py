# utils.py

import os
import shutil
import logging
import asyncio
import traceback
import torch
from pathlib import Path
from typing import Optional, Callable
from huggingface_hub import (
    HfApi, 
    snapshot_download,
    model_info,
    login
)
from llama_cpp import Llama
from model_converter import convert_model_to_float8, convert_model_to_int8
import platform

logger = logging.getLogger(__name__)

class DownloadTracker:
    """다운로드 진행 상황을 추적하는 클래스"""
    def __init__(self, total_size: int, progress_callback: Optional[Callable] = None):
        self.total_size = total_size
        self.current_size = 0
        self.progress_callback = progress_callback

    def update(self, chunk_size: int):
        """다운로드된 청크 크기만큼 진행률 업데이트"""
        self.current_size += chunk_size
        if self.progress_callback:
            progress = (self.current_size / self.total_size) if self.total_size > 0 else 0
            self.progress_callback(min(progress, 1.0))

def make_local_dir_name(model_id: str) -> str:
    """HuggingFace 모델 ID를 로컬 디렉토리 이름으로 변환"""
    return model_id.replace("/", "__")

def convert_folder_to_modelid(folder_name: str) -> str:
    """로컬 디렉토리 이름을 HuggingFace 모델 ID로 변환"""
    return folder_name.replace("__", "/")

def scan_local_models(root="./models", model_type=None):
    """로컬에 저장된 모델 목록을 유형별로 스캔"""
    if not os.path.isdir(root):
        os.makedirs(root, exist_ok=True)

    local_model_ids = []
    subdirs = ['transformers', 'gguf', 'mlx'] if not model_type else [model_type]
    for subdir in subdirs:
        subdir_path = os.path.join(root, subdir)
        if not os.path.isdir(subdir_path):
            continue
        for folder in os.listdir(subdir_path):
            full_path = os.path.join(subdir_path, folder)
            if os.path.isdir(full_path) and 'config.json' in os.listdir(full_path):
                model_id = convert_folder_to_modelid(folder)
                local_model_ids.append({"model_id": model_id, "model_type": subdir})
    logger.info(f"Scanned local models: {local_model_ids}")
    return local_model_ids

def get_all_local_models():
    """모든 모델 유형별 로컬 모델 목록을 가져옴"""
    models = scan_local_models()  # 모든 유형 스캔
    transformers = [m["model_id"] for m in models if m["model_type"] == "transformers"]
    gguf = [m["model_id"] for m in models if m["model_type"] == "gguf"]
    mlx = [m["model_id"] for m in models if m["model_type"] == "mlx"]
    return {
        "transformers": transformers,
        "gguf": gguf,
        "mlx": mlx
    }
    
def remove_hf_cache(model_id):
    """HuggingFace 캐시 폴더 제거"""
    if "/" in model_id:
        user, name = model_id.split("/", maxsplit=1)
        cache_dirname = f"./models/{user}__{name}"
    else:
        cache_dirname = f"./models/{model_id}"

    cache_path = os.path.join(cache_dirname, ".cache")
    if os.path.isdir(cache_path):
        logger.info(f"[*] Removing cache folder: {cache_path}")
        shutil.rmtree(cache_path)
    else:
        logger.info(f"[*] No cache folder found: {cache_path}")

async def get_model_size(repo_id: str, token: Optional[str] = None) -> int:
    """모델의 총 크기를 계산"""
    try:
        api = HfApi()
        info = await asyncio.to_thread(model_info, repo_id, token=token)
        return info.size_in_bytes or 0
    except Exception as e:
        logger.warning(f"모델 크기 계산 실패: {e}")
        return 0
    
def get_model_list_from_hf_hub():
    api=HfApi()
    api.token = login(new_session=False)
    api.list_models(sort="lastModified", direction=-1, limit=100)

def download_model_from_hf(hf_repo_id: str, target_dir: str, model_type: str = "transformers", quantization_bit: str = None) -> str:
    """
    동기식 모델 다운로드
    model_type: "transformers", "gguf", "mlx" 중 선택
    """
    if model_type not in ["transformers", "gguf", "mlx"]:
        model_type = "transformers"  # 기본값 설정

    target_base_dir = os.path.join("./models", model_type)
    os.makedirs(target_base_dir, exist_ok=True)
    target_dir = os.path.join(target_base_dir, make_local_dir_name(hf_repo_id))

    if os.path.isdir(target_dir):
        msg = f"[*] 이미 다운로드됨: {hf_repo_id} → {target_dir}"
        logger.info(msg)
        return msg

    logger.info(f"[*] 모델 '{hf_repo_id}'을(를) '{target_dir}'에 다운로드 중...")
    try:
        if model_type=="gguf":
            Llama.from_pretrained(
                repo_id=hf_repo_id,
                filename=f"*{quantization_bit}.gguf",
                local_dir=target_base_dir,
                hf=True
            )
        else:
            snapshot_download(
                repo_id=hf_repo_id,
                local_dir=target_dir,
                ignore_patterns=["*.md", ".gitattributes", "original/", "LICENSE.txt", "LICENSE"],
                local_dir_use_symlinks=False
            )
        remove_hf_cache(hf_repo_id)
        msg = f"[+] 다운로드 & 저장 완료: {target_dir}"
        logger.info(msg)
        return msg
    except Exception as e:
        error_msg = f"모델 '{hf_repo_id}' 다운로드 실패: {str(e)}"
        logger.error(f"{error_msg}\n{traceback.format_exc()}")
        return error_msg

async def download_model_from_hf_async(
    repo_id: str,
    target_dir: str = None,
    token: Optional[str] = None,
    progress_callback: Optional[Callable] = None
) -> str:
    """
    비동기식 모델 다운로드 (진행률 표시 지원)
    """
    try:
        # 기본 저장 경로 설정
        if not target_dir:
            target_dir = os.path.join("./models", make_local_dir_name(repo_id))
            
        # 이미 다운로드된 경우 확인
        if os.path.exists(target_dir) and any(os.scandir(target_dir)):
            return f"모델이 이미 존재합니다: {target_dir}"
            
        # 저장 경로 생성
        os.makedirs(target_dir, exist_ok=True)
        
        # 모델 크기 계산
        total_size = await get_model_size(repo_id, token)
        if total_size == 0:
            logger.warning("모델 크기를 확인할 수 없습니다. 진행률이 정확하지 않을 수 있습니다.")
        
        # 다운로드 트래커 초기화
        tracker = DownloadTracker(total_size, progress_callback)
        
        # 실제 다운로드 수행
        snapshot_path = await asyncio.to_thread(
            snapshot_download,
            repo_id=repo_id,
            local_dir=target_dir,
            token=token,
            local_dir_use_symlinks=False,
            resume_download=True,
            ignore_patterns=["*.md", ".gitattributes", "original/", "LICENSE.txt", "LICENSE"],
        )
        
        # 다운로드 완료 후 정리
        remove_hf_cache(repo_id)
        if progress_callback:
            progress_callback(1.0)  # 100% 완료 표시
            
        # 결과 메시지 생성
        model_name = repo_id.split("/")[-1]
        size_gb = total_size / (1024**3)
        message = f"""
            다운로드 완료!
            - 모델: {model_name}
            - 저장 위치: {target_dir}
            - 크기: {size_gb:.2f} GB
            """
        logger.info(f"모델 다운로드 완료: {repo_id}")
        return message.strip()
        
    except Exception as e:
        error_msg = f"다운로드 중 오류 발생: {str(e)}"
        logger.error(f"{error_msg}\n{traceback.format_exc()}")
        raise RuntimeError(error_msg)

def ensure_model_available(model_id, local_model_path=None, model_type=None):
    """
    모델의 가용성을 확인하고, 필요 시 다운로드합니다.
    
    Args:
        model_id (str): 모델의 ID.
        local_model_path (str, optional): 모델이 저장된 로컬 경로.
        model_type (str, optional): 모델 유형 (예: 'transformers', 'gguf', 'mlx').
    
    Returns:
        bool: 모델이 사용 가능하면 True, 아니면 False.
    """
    # model_type을 사용하여 모델 경로 결정 또는 다운로드 로직 수정
    if model_type:
        model_dir = os.path.join("./models", model_type, make_local_dir_name(model_id))
    else:
        model_dir = os.path.join("./models", "transformers", make_local_dir_name(model_id))
    
    if not os.path.exists(model_dir):
        try:
            download_model_from_hf(model_id, model_dir, model_type=model_type)
            return True
        except Exception as e:
            logger.error(f"모델 다운로드 실패: {e}")
            return False
    return True
    
def get_terminators(tokenizer):
    """
    모델별 종료 토큰 ID를 반환하는 함수
    """
    if "glm" in str(tokenizer.__class__).lower():
        # GLM 모델용 특수 처리
        return [tokenizer.eos_token_id]  # GLM의 EOS 토큰 사용
    else:
        # 기존 다른 모델들을 위한 처리
        return [
            tokenizer.convert_tokens_to_ids("<|end_of_text|>"),
            tokenizer.convert_tokens_to_ids("<|eot_id|>"),
            tokenizer.eos_token_id if hasattr(tokenizer, 'eos_token_id') else None
        ]
        
def convert_and_save(model_id, output_dir, push_to_hub, quant_type, model_type="transformers"):
    if not model_id:
        return "모델 ID를 입력해주세요."

    base_output_dir = os.path.join("./models", model_type)
    os.makedirs(base_output_dir, exist_ok=True)

    if quant_type == 'float8':
        if not output_dir:
            output_dir = os.path.join(base_output_dir, f"{model_id.replace('/', '__')}-float8")
        if platform.system() == 'Darwin':
            return "MacOS에서는 float8 변환을 지원하지 않습니다."
        else:
            success = convert_model_to_float8(model_id, output_dir, push_to_hub)
            if success:
                return f"모델이 성공적으로 8비트로 변환되었습니다: {output_dir}"
            else:
                return "모델 변환에 실패했습니다."
    elif quant_type == 'int8':
        if not output_dir:
            output_dir = os.path.join(base_output_dir, f"{model_id.replace('/', '__')}-int8")
        success = convert_model_to_int8(model_id, output_dir, push_to_hub)
        if success:
            return f"모델이 성공적으로 8비트로 변환되었습니다: {output_dir}"
        else:
            return "모델 변환에 실패했습니다."
    else:
        return "지원되지 않는 변환 유형입니다."