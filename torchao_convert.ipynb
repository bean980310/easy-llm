{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"./models/transformers/Qwen__Qwen2.5-7B-Instruct\"\n",
    "# We support int4_weight_only, int8_weight_only and int8_dynamic_activation_int8_weight\n",
    "# More examples and documentations for arguments can be found in https://github.com/pytorch/ao/tree/main/torchao/quantization#other-available-quantization-techniques\n",
    "quantization_config = TorchAoConfig(\"int4_weight_only\", group_size=128)\n",
    "quantized_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\", device_map=\"auto\", quantization_config=quantization_config)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "input_text = \"What are we having for dinner?\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cpu\")\n",
    "\n",
    "# compile the quantized model to get speedup\n",
    "import torchao\n",
    "torchao.quantization.utils.recommended_inductor_config_setter()\n",
    "quantized_model = torch.compile(quantized_model, mode=\"max-autotune\")\n",
    "\n",
    "output = quantized_model.generate(**input_ids, max_new_tokens=10)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "\n",
    "# benchmark the performance\n",
    "import torch.utils.benchmark as benchmark\n",
    "\n",
    "def benchmark_fn(f, *args, **kwargs):\n",
    "    # Manual warmup\n",
    "    for _ in range(5):\n",
    "        f(*args, **kwargs)\n",
    "        \n",
    "    t0 = benchmark.Timer(\n",
    "        stmt=\"f(*args, **kwargs)\",\n",
    "        globals={\"args\": args, \"kwargs\": kwargs, \"f\": f},\n",
    "        num_threads=torch.get_num_threads(),\n",
    "    )\n",
    "    return f\"{(t0.blocked_autorange().mean):.3f}\"\n",
    "\n",
    "MAX_NEW_TOKENS = 1000\n",
    "print(\"int4wo-128 model:\", benchmark_fn(quantized_model.generate, **input_ids, max_new_tokens=MAX_NEW_TOKENS))\n",
    "\n",
    "bf16_model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"cuda\", torch_dtype=torch.bfloat16)\n",
    "bf16_model = torch.compile(bf16_model, mode=\"max-autotune\")\n",
    "print(\"bf16 model:\", benchmark_fn(bf16_model.generate, **input_ids, max_new_tokens=MAX_NEW_TOKENS))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
