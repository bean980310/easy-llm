import os
import shutil
import traceback
import torch
import gradio as gr
from transformers import AutoTokenizer, AutoModelForCausalLM
import openai
import logging
from logging.handlers import RotatingFileHandler
from model_handlers import (
    MiniCPMLlama3V25Handler, GLM4Handler, GLM4VHandler, VisionModelHandler,
    Aya23Handler, GLM4HfHandler, OtherModelHandler
)
from utils import (
    make_local_dir_name,
    scan_local_models,
    download_model_from_hf,
    ensure_model_available,
    convert_and_save
)
from cache import models_cache 
##########################################
# 1) ìœ í‹¸ í•¨ìˆ˜ë“¤
##########################################

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# ë¡œê·¸ í¬ë§· ì •ì˜
formatter = logging.Formatter(
    fmt='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

# ì½˜ì†” í•¸ë“¤ëŸ¬ ì¶”ê°€
console_handler = logging.StreamHandler()
console_handler.setFormatter(formatter)
logger.addHandler(console_handler)

# íŒŒì¼ í•¸ë“¤ëŸ¬ ì¶”ê°€ (ë¡œí…Œì´íŒ… íŒŒì¼ í•¸ë“¤ëŸ¬ ì‚¬ìš©)
log_file = "app.log"  # ì›í•˜ëŠ” ë¡œê·¸ íŒŒì¼ ê²½ë¡œë¡œ ë³€ê²½ ê°€ëŠ¥
rotating_file_handler = RotatingFileHandler(
    log_file, maxBytes=5*1024*1024, backupCount=5  # 5MBë§ˆë‹¤ ìƒˆë¡œìš´ íŒŒì¼ë¡œ êµì²´, ìµœëŒ€ 5ê°œ ë°±ì—…
)
rotating_file_handler.setFormatter(formatter)
logger.addHandler(rotating_file_handler)

# ë©”ëª¨ë¦¬ ìƒì— ë¡œë“œëœ ëª¨ë¸ë“¤ì„ ì €ì¥í•˜ëŠ” ìºì‹œ
LOCAL_MODELS_ROOT = "./models"

def build_model_cache_key(model_id: str, model_type: str, local_path: str = None) -> str:
    """
    models_cacheì— ì‚¬ìš©ë  keyë¥¼ êµ¬ì„±.
    - ë§Œì•½ model_id == 'Local (Custom Path)' ì´ê³  local_pathê°€ ì£¼ì–´ì§€ë©´ 'local::{local_path}'
    - ê·¸ ì™¸ì—ëŠ” 'auto::{model_type}::{local_dir}::hf::{model_id}' í˜•íƒœ.
    """
    if model_id == "Local (Custom Path)" and local_path:
        return f"local::{local_path}"
    elif "gpt" in model_id:
        return f"api::{model_id}"
    else:
        local_dirname = make_local_dir_name(model_id)
        local_dirpath = os.path.join("./models", model_type, local_dirname)
        return f"auto::{model_type}::{local_dirpath}::hf::{model_id}"

def clear_model_cache(model_id: str, local_path: str = None) -> str:
    """
    íŠ¹ì • ëª¨ë¸ì— ëŒ€í•œ ìºì‹œë¥¼ ì œê±° (models_cacheì—ì„œ í•´ë‹¹ keyë¥¼ ì‚­ì œ).
    - ë§Œì•½ í•´ë‹¹ keyê°€ ì—†ìœ¼ë©´ 'ì´ë¯¸ ì—†ìŒ' ë©”ì‹œì§€ ë°˜í™˜
    - ì„±ê³µ ì‹œ 'ìºì‹œ ì‚­ì œ ì™„ë£Œ' ë©”ì‹œì§€
    """
    key = build_model_cache_key(model_id, local_path)
    if key in models_cache:
        del models_cache[key]
        msg = f"[cache] ëª¨ë¸ ìºì‹œ ì œê±°: {key}"
        logger.info(msg)
        return msg
    else:
        msg = f"[cache] ì´ë¯¸ ìºì‹œì— ì—†ê±°ë‚˜, ë¡œë“œëœ ì  ì—†ìŒ: {key}"
        logger.info(msg)
        return msg
    
def clear_all_model_cache():
    """
    í˜„ì¬ ë©”ëª¨ë¦¬ì— ë¡œë“œëœ ëª¨ë“  ëª¨ë¸ ìºì‹œ(models_cache)ë¥¼ í•œ ë²ˆì— ì‚­ì œ.
    í•„ìš”í•˜ë‹¤ë©´, ë¡œì»¬ í´ë”ì˜ .cacheë“¤ë„ ì¼ê´„ ì‚­ì œí•  ìˆ˜ ìˆìŒ.
    """
    # 1) ë©”ëª¨ë¦¬ ìºì‹œ ì „ë¶€ ì‚­ì œ
    count = len(models_cache)
    models_cache.clear()
    logger.info(f"[*] ë©”ëª¨ë¦¬ ìºì‹œ ì‚­ì œ: {count}ê°œ ëª¨ë¸")

    # 2) (ì„ íƒ) ë¡œì»¬ í´ë” .cache ì‚­ì œ
    #    ì˜ˆ: ./models/*/.cache í´ë” ì „ë¶€ ì‚­ì œ
    #    ì›ì¹˜ ì•Šìœ¼ë©´ ì£¼ì„ì²˜ë¦¬
    cache_deleted = 0
    for folder in os.listdir(LOCAL_MODELS_ROOT):
        folder_path = os.path.join(LOCAL_MODELS_ROOT, folder)
        if os.path.isdir(folder_path):
            cache_path = os.path.join(folder_path, ".cache")
            if os.path.isdir(cache_path):
                shutil.rmtree(cache_path)
                cache_deleted += 1
    logger.info(f"[*] ë¡œì»¬ í´ë” .cache ì‚­ì œ: {cache_deleted}ê°œ í´ë” ì‚­ì œ")
    return f"[cache all] {count}ê°œ ëª¨ë¸ ìºì‹œ ì‚­ì œ ì™„ë£Œ. ë¡œì»¬ í´ë” .cache {cache_deleted}ê°œ ì‚­ì œ."

##########################################
# 2) ëª¨ë¸ ë¡œë“œ & ì¶”ë¡  ë¡œì§
##########################################

def get_terminators(tokenizer):
    """
    ëª¨ë¸ë³„ ì¢…ë£Œ í† í° IDë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
    """
    if "glm" in str(tokenizer.__class__).lower():
        # GLM ëª¨ë¸ìš© íŠ¹ìˆ˜ ì²˜ë¦¬
        return [tokenizer.eos_token_id]  # GLMì˜ EOS í† í° ì‚¬ìš©
    else:
        # ê¸°ì¡´ ë‹¤ë¥¸ ëª¨ë¸ë“¤ì„ ìœ„í•œ ì²˜ë¦¬
        return [
            tokenizer.convert_tokens_to_ids("<|end_of_text|>"),
            tokenizer.convert_tokens_to_ids("<|eot_id|>"),
            tokenizer.eos_token_id if hasattr(tokenizer, 'eos_token_id') else None
        ]
# app.py

def load_model(model_id, model_type, local_model_path=None, api_key=None):
    """
    ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜. íŠ¹ì • ëª¨ë¸ì— ëŒ€í•œ ë¡œë“œ ë¡œì§ì„ ì™¸ë¶€ í•¸ë“¤ëŸ¬ë¡œ ë¶„ë¦¬.
    """
    if model_type not in ["transformers", "gguf", "mlx"]:
        logger.error(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ëª¨ë¸ ìœ í˜•: {model_type}")
        return None
    if model_id == "openbmb/MiniCPM-Llama3-V-2_5":
        # ëª¨ë¸ ì¡´ì¬ í™•ì¸ ë° ë‹¤ìš´ë¡œë“œ
        if not ensure_model_available(model_id, local_model_path):
            logger.error(f"ëª¨ë¸ '{model_id}'ì„(ë¥¼) ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
        handler = MiniCPMLlama3V25Handler(model_dir=local_model_path or f"./models/{make_local_dir_name(model_id)}")
        models_cache[model_id] = handler
        return handler
    elif model_id in [
        "Bllossom/llama-3.2-Korean-Bllossom-AICA-5B",
    ] or "vision" in model_id.lower() and model_id != "Bllossom/llama-3.1-Korean-Bllossom-Vision-8B":
        # ëª¨ë¸ ì¡´ì¬ í™•ì¸ ë° ë‹¤ìš´ë¡œë“œ
        if not ensure_model_available(model_id, local_model_path):
            logger.error(f"ëª¨ë¸ '{model_id}'ì„(ë¥¼) ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
        handler = VisionModelHandler(model_dir=local_model_path or f"./models/{make_local_dir_name(model_id)}")
        models_cache[model_id] = handler
        return handler
    elif model_id == "THUDM/glm-4v-9b":
        # ëª¨ë¸ ì¡´ì¬ í™•ì¸ ë° ë‹¤ìš´ë¡œë“œ
        if not ensure_model_available(model_id, local_model_path):
            logger.error(f"ëª¨ë¸ '{model_id}'ì„(ë¥¼) ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
        handler = GLM4VHandler(model_dir=local_model_path or f"./models/{make_local_dir_name(model_id)}")
        models_cache[model_id] = handler
        return handler
    elif model_id == "THUDM/glm-4-9b-chat":
        if not ensure_model_available(model_id, local_model_path):
            logger.error(f"ëª¨ë¸ '{model_id}'ì„(ë¥¼) ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
        handler = GLM4Handler(model_dir=local_model_path or f"./models/{make_local_dir_name(model_id)}")
        models_cache[model_id] = handler
        return handler
    elif model_id in ["THUDM/glm-4-9b-chat-hf", "THUDM/glm-4-9b-chat-1m-hf"]:
        if not ensure_model_available(model_id, local_model_path):
            logger.error(f"ëª¨ë¸ '{model_id}'ì„(ë¥¼) ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
        handler = GLM4HfHandler(model_dir=local_model_path or f"./models/{make_local_dir_name(model_id)}")
        models_cache[model_id] = handler
        return handler
    elif model_id in ["bean980310/glm-4-9b-chat-hf_float8", "genai-archive/glm-4-9b-chat-hf_int8"]:
        # 'fp8' íŠ¹í™” í•¸ë“¤ëŸ¬ ë¡œì§ ì¶”ê°€
        if not ensure_model_available(model_id, local_model_path):
            logger.error(f"ëª¨ë¸ '{model_id}'ì„(ë¥¼) ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
        handler = GLM4HfHandler(model_dir=local_model_path or f"./models/{make_local_dir_name(model_id)}")
        models_cache[model_id] = handler
        return handler
    elif model_id == "CohereForAI/aya-23-8B" or model_id == "CohereForAI/aya-23-35B":
        if not ensure_model_available(model_id, local_model_path):
            logger.error(f"ëª¨ë¸ '{model_id}'ì„(ë¥¼) ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
        handler = Aya23Handler(model_dir=local_model_path or f"./models/{make_local_dir_name(model_id)}")
        models_cache[model_id] = handler
        return handler
    else:
        if not ensure_model_available(model_id, local_model_path):
            logger.error(f"ëª¨ë¸ '{model_id}'ì„(ë¥¼) ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
        handler = OtherModelHandler(model_id, local_model_path=local_model_path, model_type=model_type)
        models_cache[model_id] = handler
        return handler

# app.py

def generate_answer(history, selected_model, model_type, local_model_path=None, image_input=None, api_key=None):
    """
    ì‚¬ìš©ì íˆìŠ¤í† ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ ìƒì„±.
    """
    cache_key = build_model_cache_key(selected_model, model_type, local_path=local_model_path)
    model_cache = models_cache.get(cache_key, {})
    
    if "gpt" in selected_model:
        if not api_key:
            logger.error("OpenAI API Key is missing.")
            return "OpenAI API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤."
        openai.api_key = api_key
        messages = [{"role": msg['role'], "content": msg['content']} for msg in history]
        logger.info(f"[*] OpenAI API ìš”ì²­: {messages}")
        
        try:
            response = openai.ChatCompletion.create(
                model=selected_model,
                messages=messages,
                temperature=0.7,
                max_tokens=1024,
                top_p=0.9
            )
            answer = response.choices[0].message["content"]
            logger.info(f"[*] OpenAI ì‘ë‹µ: {answer}")
            return answer
        except Exception as e:
            logger.error(f"OpenAI API ì˜¤ë¥˜: {str(e)}\n\n{traceback.format_exc()}")
            return f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}\n\n{traceback.format_exc()}"
    
    elif selected_model in [
        "Bllossom/llama-3.2-Korean-Bllossom-AICA-5B",
    ] or "vision" in selected_model.lower():
        handler: VisionModelHandler = models_cache.get(selected_model)
        if not handler:
            logger.info(f"[*] ëª¨ë¸ ë¡œë“œ ì¤‘: {selected_model}")
            handler = load_model(selected_model, local_model_path=local_model_path)
        
        if not handler:
            logger.error("ëª¨ë¸ í•¸ë“¤ëŸ¬ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return "ëª¨ë¸ í•¸ë“¤ëŸ¬ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        logger.info(f"[*] Generating answer using VisionModelHandler")
        answer = handler.generate_answer(history, image_input)
        return answer
    
    elif selected_model == "openbmb/MiniCPM-Llama3-V-2_5":
        handler: MiniCPMLlama3V25Handler = models_cache.get(selected_model)
        if not handler:
            logger.info(f"[*] ëª¨ë¸ ë¡œë“œ ì¤‘: {selected_model}")
            handler = load_model(selected_model, local_model_path=local_model_path)
        
        if not handler:
            logger.error("ëª¨ë¸ í•¸ë“¤ëŸ¬ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return "ëª¨ë¸ í•¸ë“¤ëŸ¬ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        logger.info(f"[*] Generating answer using MiniCPMLlama3V25Handler")
        # image_input íŒŒë¼ë¯¸í„° ì „ë‹¬ ì¶”ê°€
        logger.info(f"[*] Image input provided: {image_input is not None}")
        answer = handler.generate_answer(history, image_input=image_input)
        return answer
    
    elif selected_model == "THUDM/glm-4v-9b":
        handler: GLM4VHandler = models_cache.get(selected_model)
        if not handler:
            logger.info(f"[*] ëª¨ë¸ ë¡œë“œ ì¤‘: {selected_model}")
            handler = load_model(selected_model, local_model_path=local_model_path)

        if not handler:
            logger.error("ëª¨ë¸ í•¸ë“¤ëŸ¬ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return "ëª¨ë¸ í•¸ë“¤ëŸ¬ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

        logger.info(f"[*] Generating answer using GLM4VHandler")
        answer = handler.generate_answer(history)
        return answer
    elif selected_model == "THUDM/glm-4-9b-chat":
        handler: GLM4Handler = models_cache.get(selected_model)
        if not handler:
            logger.info(f"[*] ëª¨ë¸ ë¡œë“œ ì¤‘: {selected_model}")
            handler = load_model(selected_model, local_model_path=local_model_path)

        if not handler:
            logger.error("ëª¨ë¸ í•¸ë“¤ëŸ¬ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return "ëª¨ë¸ í•¸ë“¤ëŸ¬ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

        logger.info(f"[*] Generating answer using GLM4Handler")
        answer = handler.generate_answer(history)
        return answer
    elif selected_model in ["THUDM/glm-4-9b-chat-hf", "THUDM/glm-4-9b-chat-1m-hf", "bean980310/glm-4-9b-chat-hf_float8", "genai-archive/glm-4-9b-chat-hf_int8"] :
        handler: GLM4HfHandler = models_cache.get(selected_model)
        if not handler:
            logger.info(f"[*] ëª¨ë¸ ë¡œë“œ ì¤‘: {selected_model}")
            handler = load_model(selected_model, local_model_path=local_model_path)

        if not handler:
            logger.error("ëª¨ë¸ í•¸ë“¤ëŸ¬ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return "ëª¨ë¸ í•¸ë“¤ëŸ¬ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

        logger.info(f"[*] Generating answer using GLM4Handler")
        answer = handler.generate_answer(history)
        return answer
    elif selected_model in ["CohereForAI/aya-23-8B", "CohereForAI/aya-23-35B"]:
        handler: Aya23Handler = models_cache.get(selected_model)
        if not handler:
            logger.info(f"[*] ëª¨ë¸ ë¡œë“œ ì¤‘: {selected_model}")
            handler = load_model(selected_model, local_model_path=local_model_path)

        if not handler:
            logger.error("ëª¨ë¸ í•¸ë“¤ëŸ¬ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return "ëª¨ë¸ í•¸ë“¤ëŸ¬ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        logger.info(f"[*] Generating answer using Aya23Handler")
        answer = handler.generate_answer(history)
        return answer
    else:
        handler: OtherModelHandler = models_cache.get(selected_model)
        if not handler:
            logger.info(f"[*] ëª¨ë¸ ë¡œë“œ ì¤‘: {selected_model}")
            handler = load_model(selected_model, local_model_path=local_model_path)

        if not handler:
            logger.error("ëª¨ë¸ í•¸ë“¤ëŸ¬ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return "ëª¨ë¸ í•¸ë“¤ëŸ¬ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        logger.info(f"[*] Generating answer using default handler for model: {selected_model}")
        answer = handler.generate_answer(history)
        return answer

##########################################
# 3) Gradio UI
##########################################

with gr.Blocks() as demo:
    gr.Markdown("## ê°„ë‹¨í•œ Chatbot")
    known_hf_models = [
        "meta-llama/Llama-3.1-8B",
        "meta-llama/Llama-3.2-11B-Vision",
        "meta-llama/Llama-3.2-11B-Vision-Instruct",
        "openbmb/MiniCPM-Llama3-V-2_5",
        "Bllossom/llama-3.2-Korean-Bllossom-3B",
        "Bllossom/llama-3.2-Korean-Bllossom-AICA-5B",
        "Bllossom/llama-3.1-Korean-Bllossom-Vision-8B",
        "THUDM/glm-4-9b-chat",
        "THUDM/glm-4-9b-chat-hf",
        "THUDM/glm-4-9b-chat-1m",
        "THUDM/glm-4-9b-chat-1m-hf",
        "THUDM/glm-4v-9b",
        "bean980310/glm-4-9b-chat-hf_float8",
        "huggyllama/llama-7b",
        "OrionStarAI/Orion-14B-Base",
        "OrionStarAI/Orion-14B-Chat",
        "OrionStarAI/Orion-14B-LongChat",
        "CohereForAI/aya-23-8B",
        "CohereForAI/aya-23-35B",
        "Qwen/Qwen2.5-7B",
        "Qwen/Qwen2.5-7B-Instruct",
        "Qwen/Qwen2.5-14B",
        "Qwen/Qwen2.5-14B-Instruct",
        "EleutherAI/polyglot-ko-1.3b",
        "gpt-4o-mini",
        "gpt-4o",
        "gpt-3.5-turbo"
    ]
    with gr.Tab("ë©”ì¸"):
        local_model_folders = scan_local_models()
        initial_choices = known_hf_models + local_model_folders + ["Local (Custom Path)"]
        initial_choices = list(dict.fromkeys(initial_choices))

        with gr.Row():
            model_type_dropdown = gr.Radio(
                label="ëª¨ë¸ ìœ í˜• ì„ íƒ",
                choices=["transformers", "gguf", "mlx"],
                value="transformers",
                inline=True
            )
            
        model_dropdown = gr.Dropdown(
            label="ëª¨ë¸ ì„ íƒ",
            choices=initial_choices,
            value=initial_choices[0] if len(initial_choices) > 0 else None,
        )
        local_path_text = gr.Textbox(
            label="(Local Path) ë¡œì»¬ í´ë” ê²½ë¡œ",
            placeholder="./models/my-llama",
            visible=False  # ê¸°ë³¸ì ìœ¼ë¡œ ìˆ¨ê¹€
        )
        api_key_text = gr.Textbox(
            label="OpenAI API Key",
            placeholder="sk-...",
            visible=False  # ê¸°ë³¸ì ìœ¼ë¡œ ìˆ¨ê¹€
        )
        image_info = gr.Markdown("", visible=False)
        with gr.Column():
            # image_inputì„ ë¨¼ì € ì •ì˜í•©ë‹ˆë‹¤.
            with gr.Row():
                image_input = gr.Image(label="ì´ë¯¸ì§€ ì—…ë¡œë“œ (ì„ íƒ)", type="pil", visible=False)  # ì´ˆê¸° ìƒíƒœ ìˆ¨ê¹€
                chatbot = gr.Chatbot(height=400, label="Chatbot", type="messages")  # 'type' íŒŒë¼ë¯¸í„° ì„¤ì •
            with gr.Row():
                msg = gr.Textbox(
                    label="ë©”ì‹œì§€ ì…ë ¥",
                    placeholder="ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”...",
                    scale=9  # 90% ì°¨ì§€
                )
                send_btn = gr.Button(
                    "ì „ì†¡",
                    scale=1,  # 10% ì°¨ì§€
                    variant="primary"
                )
            with gr.Row():
                status_text = gr.Markdown("", elem_id="status_text")
        history_state = gr.State([])
        
        def toggle_api_key_display(selected_model):
            """
            OpenAI API Key ì…ë ¥ í•„ë“œì™€ ë¡œì»¬ ê²½ë¡œ ì…ë ¥ í•„ë“œì˜ ê°€ì‹œì„±ì„ ì œì–´í•©ë‹ˆë‹¤.
            """
            api_visible = "gpt" in selected_model
            local_path_visible = selected_model == "Local (Custom Path)"
            return gr.update(visible=api_visible), gr.update(visible=local_path_visible)
    
        def toggle_image_input(selected_model):
            """
            ì´ë¯¸ì§€ ì—…ë¡œë“œ í•„ë“œì™€ ì •ë³´ ë©”ì‹œì§€ì˜ ê°€ì‹œì„±ì„ ì œì–´í•©ë‹ˆë‹¤.
            """
            requires_image = (
                "vision" in selected_model.lower() or
                selected_model in [
                    "Bllossom/llama-3.2-Korean-Bllossom-AICA-5B",
                    "THUDM/glm-4v-9b",
                    "openbmb/MiniCPM-Llama3-V-2_5"
                ]
            )
            if requires_image:
                return gr.update(visible=True), "ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."
            else:
                return gr.update(visible=False), "ì´ë¯¸ì§€ ì…ë ¥ì´ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
    
        def update_model_dropdown(model_type, _):
            """
            ëª¨ë¸ ìœ í˜•ì— ë”°ë¼ ëª¨ë¸ ë“œë¡­ë‹¤ìš´ì˜ ì„ íƒì§€ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
            """
            return scan_local_models(model_type=model_type)

        model_type_dropdown.change(
            fn=update_model_dropdown,
            inputs=[model_type_dropdown, model_dropdown],
            outputs=[model_dropdown]
        )

        model_dropdown.change(
            fn=toggle_api_key_display,
            inputs=[model_dropdown],
            outputs=[api_key_text, local_path_text]
        ).then(
            fn=toggle_image_input,
            inputs=[model_dropdown],
            outputs=[image_input, image_info]  # ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì‚¬ìš©
        )
        def user_message(user_input, history):
            if not user_input.strip():
                return "", history, ""
            history = history + [{"role": "user", "content": user_input}]
            return "", history, "ğŸ¤” ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."

        def bot_message(history, selected_model, local_model_path, image, api_key):
            try:
                answer = generate_answer(history, selected_model, local_model_path, image, api_key)
            except Exception as e:
                answer = f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}\n\n{traceback.format_exc()}"
            history = history + [{"role": "assistant", "content": answer}]
            return history, ""  # ë¡œë”© ìƒíƒœ ì œê±°

        # ë©”ì‹œì§€ ì „ì†¡ ì‹œ í•¨ìˆ˜ ì—°ê²°
        msg.submit(
            fn=user_message,
            inputs=[msg, history_state],
            outputs=[msg, history_state, status_text],
            queue=False  # ì‚¬ìš©ì ì…ë ¥ì€ ì¦‰ì‹œ ì²˜ë¦¬
        ).then(
            fn=bot_message,
            inputs=[history_state, model_dropdown, local_path_text, image_input, api_key_text],
            outputs=[history_state, status_text],
            queue=True  # ëª¨ë¸ ìƒì„±ì€ íì—ì„œ ì²˜ë¦¬
        ).then(
            fn=lambda h: h,
            inputs=history_state,
            outputs=chatbot,
            queue=False  # UI ì—…ë°ì´íŠ¸ëŠ” ì¦‰ì‹œ ì²˜ë¦¬
        )
        send_btn.click(
            fn=user_message,
            inputs=[msg, history_state],
            outputs=[msg, history_state, status_text],
            queue=False
        ).then(
            fn=bot_message,
            inputs=[history_state, model_dropdown, local_path_text, image_input, api_key_text],
            outputs=[history_state, status_text],
            queue=True
        ).then(
            fn=lambda h: h,
            inputs=history_state,
            outputs=chatbot,
            queue=False
        )
    with gr.Tab("ë‹¤ìš´ë¡œë“œ"):
        gr.Markdown("""### ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
        HuggingFaceì—ì„œ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  ë¡œì»¬ì— ì €ì¥í•©ë‹ˆë‹¤. 
        ë¯¸ë¦¬ ì •ì˜ëœ ëª¨ë¸ ëª©ë¡ì—ì„œ ì„ íƒí•˜ê±°ë‚˜, ì»¤ìŠ¤í…€ ëª¨ë¸ IDë¥¼ ì§ì ‘ ì…ë ¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.""")
        
        with gr.Column():
            # ë‹¤ìš´ë¡œë“œ ëª¨ë“œ ì„ íƒ (ë¼ë””ì˜¤ ë²„íŠ¼ì„ ì„¸ê·¸ë¨¼íŠ¸ë¡œ ë³€ê²½)
            download_mode = gr.Radio(
                label="ë‹¤ìš´ë¡œë“œ ë°©ì‹ ì„ íƒ",
                choices=["Predefined", "Custom Repo ID"],
                value="Predefined",
                container=True,
            )
            
            # ëª¨ë¸ ì„ íƒ/ì…ë ¥ ì˜ì—­
            with gr.Column(visible=True) as predefined_column:
                predefined_dropdown = gr.Dropdown(
                    label="ëª¨ë¸ ì„ íƒ",
                    choices=sorted(known_hf_models),
                    value=known_hf_models[0] if known_hf_models else None,
                    info="ì§€ì›ë˜ëŠ” ëª¨ë¸ ëª©ë¡ì…ë‹ˆë‹¤."
                )
                
            with gr.Column(visible=False) as custom_column:
                custom_repo_id_box = gr.Textbox(
                    label="Custom Model ID",
                    placeholder="ì˜ˆ) facebook/opt-350m",
                    info="HuggingFaceì˜ ëª¨ë¸ IDë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: organization/model-name)"
                )
                
            # ë‹¤ìš´ë¡œë“œ ì„¤ì •
            with gr.Row():
                with gr.Column(scale=2):
                    target_path = gr.Textbox(
                        label="ì €ì¥ ê²½ë¡œ",
                        placeholder="./models/my-model",
                        value="",
                        interactive=True,
                        info="ë¹„ì›Œë‘ë©´ ìë™ìœ¼ë¡œ ê²½ë¡œê°€ ìƒì„±ë©ë‹ˆë‹¤."
                    )
                with gr.Column(scale=1):
                    use_auth = gr.Checkbox(
                        label="ì¸ì¦ í•„ìš”",
                        value=False,
                        info="ë¹„ê³µê°œ ë˜ëŠ” gated ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œ ì²´í¬"
                    )
            
            with gr.Column(visible=False) as auth_column:
                hf_token = gr.Textbox(
                    label="HuggingFace Token",
                    placeholder="hf_...",
                    type="password",
                    info="HuggingFaceì—ì„œ ë°œê¸‰ë°›ì€ í† í°ì„ ì…ë ¥í•˜ì„¸ìš”."
                )
            
            # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ê³¼ ì§„í–‰ ìƒíƒœ
            with gr.Row():
                download_btn = gr.Button(
                    "ë‹¤ìš´ë¡œë“œ ì‹œì‘",
                    variant="primary",
                    scale=2
                )
                cancel_btn = gr.Button(
                    "ì·¨ì†Œ",
                    variant="stop",
                    scale=1,
                    interactive=False
                )
                
            # ìƒíƒœ í‘œì‹œ
            download_status = gr.Markdown("")
            progress_bar = gr.Progress(
                track_tqdm=True,  # tqdm progress barsë¥¼ ì¶”ì 
            )
            
            # ë‹¤ìš´ë¡œë“œ ê²°ê³¼ì™€ ë¡œê·¸
            with gr.Accordion("ìƒì„¸ ì •ë³´", open=False):
                download_info = gr.TextArea(
                    label="ë‹¤ìš´ë¡œë“œ ë¡œê·¸",
                    interactive=False,
                    max_lines=10,
                    autoscroll=True
                )

        # UI ë™ì‘ ì œì–´ë¥¼ ìœ„í•œ í•¨ìˆ˜ë“¤
        def toggle_download_mode(mode):
            """ë‹¤ìš´ë¡œë“œ ëª¨ë“œì— ë”°ë¼ UI ì»´í¬ë„ŒíŠ¸ í‘œì‹œ/ìˆ¨ê¹€"""
            return [
                gr.update(visible=(mode == "Predefined")),  # predefined_column
                gr.update(visible=(mode == "Custom Repo ID"))  # custom_column
            ]

        def toggle_auth(use_auth_val):
            """ì¸ì¦ í•„ìš” ì—¬ë¶€ì— ë”°ë¼ í† í° ì…ë ¥ì°½ í‘œì‹œ/ìˆ¨ê¹€"""
            return {
                auth_column: use_auth_val
            }

        def update_download_ui(
            status: str = "",
            btn_enabled: bool = True,
            cancel_enabled: bool = False,
            info: str = "",
            model_list = None
        ):
            """UI ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•œ í—¬í¼ í•¨ìˆ˜"""
            updates = {
                "download_status": status,
                "download_btn": gr.Button(interactive=btn_enabled),
                "cancel_btn": gr.Button(interactive=cancel_enabled),
                "download_info": info
            }
            if model_list is not None:
                updates["model_dropdown"] = gr.Dropdown(choices=model_list)
            return updates

        def download_with_progress(mode, predefined_choice, custom_repo, target_dir, use_auth_val, token, progress=gr.Progress()):
            """ì§„í–‰ë¥  í‘œì‹œì™€ í•¨ê»˜ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ìˆ˜í–‰"""
            try:
                repo_id = predefined_choice if mode == "Predefined" else custom_repo.strip()
                if not repo_id:
                    yield (
                        "âŒ ëª¨ë¸ IDë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.",  # status
                        gr.Button(interactive=True),  # download_btn
                        gr.Button(interactive=False),  # cancel_btn
                        "ë‹¤ìš´ë¡œë“œê°€ ì‹œì‘ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",  # download_info
                        None  # model_dropdown (no update)
                    )
                    return
                
                if "gguf" in repo_id.lower():
                    model_type = "gguf"
                elif "mlx" in repo_id.lower():
                    model_type = "mlx"
                else:
                    model_type = "transformers"


                # ì§„í–‰ ìƒíƒœ ì´ˆê¸°í™”
                progress(0, desc="ì¤€ë¹„ ì¤‘...")
                yield (
                    "ğŸ”„ ë‹¤ìš´ë¡œë“œ ì¤€ë¹„ ì¤‘...",  # status
                    gr.Button(interactive=False),  # download_btn
                    gr.Button(interactive=True),  # cancel_btn
                    f"ëª¨ë¸: {repo_id}\nì¤€ë¹„ ì¤‘...",  # download_info
                    None  # model_dropdown (no update)
                )

                # ì‹¤ì œ ë‹¤ìš´ë¡œë“œ ìˆ˜í–‰
                progress(0.5, desc="ë‹¤ìš´ë¡œë“œ ì¤‘...")
                result = download_model_from_hf(
                    repo_id,
                    target_dir or os.path.join("./models", model_type, make_local_dir_name(repo_id)),
                    model_type=model_type
                )


                # ë‹¤ìš´ë¡œë“œ ì™„ë£Œ í›„ UI ì—…ë°ì´íŠ¸
                progress(1.0, desc="ì™„ë£Œ")
                yield (
                    "âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!" if "ì‹¤íŒ¨" not in result else "âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨",  # status
                    gr.Button(interactive=True),  # download_btn
                    gr.Button(interactive=False),  # cancel_btn
                    result,  # download_info
                    gr.Dropdown(choices=scan_local_models())  # model_dropdown update
                )

            except Exception as e:
                yield (
                    "âŒ ì˜¤ë¥˜ ë°œìƒ",  # status
                    gr.Button(interactive=True),  # download_btn
                    gr.Button(interactive=False),  # cancel_btn
                    f"ì˜¤ë¥˜: {str(e)}\n\n{traceback.format_exc()}",  # download_info
                    None  # model_dropdown (no update)
                )
        # ì´ë²¤íŠ¸ ì—°ê²°
        download_mode.change(
            fn=toggle_download_mode,
            inputs=download_mode,
            outputs=[predefined_column, custom_column]
        )
        
        use_auth.change(
            fn=toggle_auth,
            inputs=use_auth,
            outputs=[auth_column]
        )
        
        download_btn.click(
            fn=download_with_progress,
            inputs=[
                download_mode,
                predefined_dropdown,
                custom_repo_id_box,
                target_path,
                use_auth,
                hf_token
            ],
            outputs=[
                download_status,
                download_btn,
                cancel_btn,
                download_info,
                model_dropdown
            ]
        )
    
    with gr.Tab("ìºì‹œ"):
        with gr.Row():
            with gr.Column():
                refresh_button = gr.Button("ëª¨ë¸ ëª©ë¡ ìƒˆë¡œê³ ì¹¨")
                refresh_info = gr.Textbox(label="ìƒˆë¡œê³ ì¹¨ ê²°ê³¼", interactive=False)
            with gr.Column():
                clear_all_btn = gr.Button("ëª¨ë“  ëª¨ë¸ ìºì‹œ ì‚­ì œ")
                clear_all_result = gr.Textbox(label="ê²°ê³¼", interactive=False)

        def refresh_model_list():
            """
            ìˆ˜ë™ ìƒˆë¡œê³ ì¹¨ ì‹œ í˜¸ì¶œë˜ëŠ” í•¨ìˆ˜.
            - ìƒˆë¡œ scan_local_models()
            - DropDown ëª¨ë¸ ëª©ë¡ ì—…ë°ì´íŠ¸
            """
            # ìƒˆë¡œ ìŠ¤ìº”
            new_local_models = scan_local_models()
            # ìƒˆ choices: ê¸°ì¡´ HF ëª¨ë¸ + ìƒˆ local ëª¨ë¸ + Local (Custom Path)
            new_choices = known_hf_models + new_local_models + ["Local (Custom Path)"]
            new_choices = list(dict.fromkeys(new_choices))
            # ë°˜í™˜ê°’:
            logger.info("ëª¨ë¸ ëª©ë¡ ìƒˆë¡œê³ ì¹¨")
            return gr.update(choices=new_choices), "ëª¨ë¸ ëª©ë¡ì„ ìƒˆë¡œê³ ì¹¨ í–ˆìŠµë‹ˆë‹¤."
            
        refresh_button.click(
            fn=refresh_model_list,
            inputs=[],
            outputs=[model_dropdown, refresh_info]
        )
        clear_all_btn.click(
            fn=clear_all_model_cache,
            inputs=[],
            outputs=clear_all_result
        )
    with gr.Tab("ìœ í‹¸ë¦¬í‹°"):
        gr.Markdown("### ëª¨ë¸ ë¹„íŠ¸ ë³€í™˜ê¸°")
        gr.Markdown("Transformersì™€ BitsAndBytesë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ 8ë¹„íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.")
        
        with gr.Row():
            model_id = gr.Textbox(label="HuggingFace ëª¨ë¸ ID", placeholder="ì˜ˆ: gpt2")
            output_dir = gr.Textbox(label="ì €ì¥ ë””ë ‰í† ë¦¬", placeholder="./converted_models/gpt2_8bit")
        with gr.Row():
            quant_type = gr.Radio(choices=["float8", "int8"], label="ë³€í™˜ ìœ í˜•", value="int8")
        with gr.Row():
            push_to_hub = gr.Checkbox(label="Hugging Face Hubì— í‘¸ì‹œ", value=False)
        
        convert_button = gr.Button("ëª¨ë¸ ë³€í™˜ ì‹œì‘")
        output = gr.Textbox(label="ê²°ê³¼")
        
        convert_button.click(fn=convert_and_save, inputs=[model_id, output_dir, push_to_hub, quant_type], outputs=output)

demo.launch(debug=True, inbrowser=True, server_port=7861)