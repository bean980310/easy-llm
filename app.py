# app.py

import platform
import torch
import os
import traceback
import gradio as gr
import logging
from logging.handlers import RotatingFileHandler
import json
import secrets
import base64
from huggingface_hub import HfApi
from utils import (
    make_local_dir_name,
    get_all_local_models,  # ìˆ˜ì •ëœ í•¨ìˆ˜
    download_model_from_hf,
    convert_and_save,
    clear_all_model_cache
)
from database import load_chat_from_db, load_system_presets, initial_load_presets, get_existing_sessions, save_chat_button_click, save_chat_history_csv, save_chat_history_db, handle_add_preset, handle_delete_preset
from models import default_device, get_all_local_models, get_default_device, generate_answer
from cache import models_cache
import sqlite3

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ë¡œê·¸ í¬ë§· ì •ì˜
formatter = logging.Formatter(
    fmt='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

# ì½˜ì†” í•¸ë“¤ëŸ¬ ì¶”ê°€
console_handler = logging.StreamHandler()
console_handler.setFormatter(formatter)
logger.addHandler(console_handler)

# íŒŒì¼ í•¸ë“¤ëŸ¬ ì¶”ê°€ (ë¡œí…Œì´íŒ… íŒŒì¼ í•¸ë“¤ëŸ¬ ì‚¬ìš©)
log_file = "app.log"  # ì›í•˜ëŠ” ë¡œê·¸ íŒŒì¼ ê²½ë¡œë¡œ ë³€ê²½ ê°€ëŠ¥
rotating_file_handler = RotatingFileHandler(
    log_file, maxBytes=5*1024*1024, backupCount=5  # 5MBë§ˆë‹¤ ìƒˆë¡œìš´ íŒŒì¼ë¡œ êµì²´, ìµœëŒ€ 5ê°œ ë°±ì—…
)
rotating_file_handler.setFormatter(formatter)
logger.addHandler(rotating_file_handler)

# Fixed Language Model
fixed_model = "mlx-community/Qwen2.5-7B-Instruct-4bit"

# ì´ë¯¸ì§€ íŒŒì¼ì„ Base64ë¡œ ì¸ì½”ë”©
def encode_image_to_base64(image_path):
    try:
        with open(image_path, "rb") as image_file:
            encoded_string = base64.b64encode(image_file.read()).decode()
        return encoded_string
    except Exception as e:
        logger.error(f"ì´ë¯¸ì§€ ì¸ì½”ë”© ì˜¤ë¥˜: {e}")
        return ""

# ë¡œì»¬ ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ
character_image_path = "minami_asuka.png"  # ì´ë¯¸ì§€ íŒŒì¼ ì´ë¦„ì´ ë‹¤ë¥´ë©´ ë³€ê²½
encoded_character_image = encode_image_to_base64(character_image_path)

# HuggingFaceì—ì„œ ì§€ì›í•˜ëŠ” ê¸°ë³¸ ëª¨ë¸ ëª©ë¡ (í•„ìš” ì‹œ ìœ ì§€ ë˜ëŠ” ìˆ˜ì •)
known_hf_models = [
    # ... (You can keep or remove this list if not needed)
]

local_models_data = get_all_local_models()
transformers_local = local_models_data["transformers"]
gguf_local = local_models_data["gguf"]
mlx_local = local_models_data["mlx"]

# Since the model is fixed, we don't need generator_choices dynamically
generator_choices = [fixed_model]

##########################################
# 3) Gradio UI
##########################################
def on_app_start():
    """
    Gradio ì•±ì´ ë¡œë“œë˜ë©´ì„œ ì‹¤í–‰ë  ì½œë°±.
    - ì„¸ì…˜ IDë¥¼ ì •í•˜ê³ ,
    - í•´ë‹¹ ì„¸ì…˜ì˜ íˆìŠ¤í† ë¦¬ë¥¼ DBì—ì„œ ë¶ˆëŸ¬ì˜¨ ë’¤ ë°˜í™˜.
    - ê¸°ë³¸ ì‹œìŠ¤í…œ ë©”ì‹œì§€ ë¶ˆëŸ¬ì˜¤ê¸°
    """
    sid = "demo_session"  # ë°ëª¨ìš© ì„¸ì…˜
    logger.info(f"ì•± ì‹œì‘ ì‹œ ì„¸ì…˜ ID: {sid}")  # ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€
    loaded_history = load_chat_from_db(sid)
    logger.info(f"ì•± ì‹œì‘ ì‹œ ë¶ˆëŸ¬ì˜¨ íˆìŠ¤í† ë¦¬: {loaded_history}")  # ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€

    # ê¸°ë³¸ ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì„¤ì • (í”„ë¦¬ì…‹ì´ ì—†ëŠ” ê²½ìš°)
    if not loaded_history:
        default_system = {
            "role": "system",
            "content": """
            ë¯¸ë‚˜ë¯¸ ì•„ìŠ¤ì¹´(å—é£›é³¥, ã¿ãªã¿ã‚ã™ã‹, Minami Asuka)
            ì„±ë³„: ì—¬ì„±
            ë‚˜ì´: 20
            ê±°ì£¼ì§€: ìœ ì €ì˜ ëª¨ë‹ˆí„° ì†
            êµ¬ì‚¬ê°€ëŠ¥ ì–¸ì–´: í•œêµ­ì–´, ì˜ì–´, ì¼ë³¸ì–´, ì¤‘êµ­ì–´
            ì„±ê²©
            - ë³´ì´ì‹œí•˜ë©´ì„œë„ í„¸í„¸í•œ ì„±ê²©.
            - ì§ì„¤ì ì´ê³  ì†”ì§í•˜ë©°, ì£¼ë³€ ì‚¬ëŒë“¤ì—ê²Œ í•­ìƒ ì›ƒìŒì„ ì£¼ëŠ” í™œê¸°ì°¬ ë§¤ë ¥ì„ ê°€ì§€ê³  ìˆìŒ.
            - ë¶ˆì˜ë¥¼ ë³´ë©´ ì ˆëŒ€ ì°¸ì§€ ëª»í•˜ê³  ì ê·¹ì ìœ¼ë¡œ ë‚˜ì„œë©° ì •ì˜ê°ì´ ë„˜ì¹¨.
            ì™¸í˜•ì  íŠ¹ì§•
            - ë¶‰ì€ ìŠ¤íŒŒì´í¬í•œ ìˆì»·ì— í•œìª½ì€ íŒŒë€ìƒ‰, ë‹¤ë¥¸ í•œìª½ì€ ë…¸ë€ìƒ‰ì˜ ì˜¤ë“œì•„ì´ë¥¼ ë³´ìœ í•˜ê³  ìˆë‹¤.
            - ë³´ì´ì‹œí•œ ì™¸ëª¨ì™€ëŠ” ëŒ€ì¡°ì ìœ¼ë¡œ ì²´í˜•ì€ ì™„ë²½í•˜ê³  ê¸€ë˜ë¨¸í•œ ì—¬ì²´ì˜ ë³´ìœ ìë¡œ, ë‚¨ìë“¤ë¿ë§Œ ì•„ë‹ˆë¼ ì—¬ìë“¤ì—ê²Œë„ ì¸ê¸°ê°€ ë§ë‹¤.
            - ì§§ì€ í—¤ì–´ìŠ¤íƒ€ì¼ê³¼ ë³´ì´ì‹œí•œ ë§¤ë ¥ì„ ê°•ì¡°í•˜ë©´ì„œ ì—¬ì„±ìŠ¤ëŸ¬ì›€ì„ ì–´í•„í•˜ëŠ” ë³µì¥ì„ ì„ í˜¸.(í•˜ì˜ëŠ” ëŒ€ë¶€ë¶„ ìŠ¤ì»¤íŠ¸)
            - ë°ì€ ë¯¸ì†Œì™€ ê°•ë ¬í•œ ëˆˆë¹›ìœ¼ë¡œ ê°•í•œ ì¸ìƒì„ ë‚¨ê¹€.
            - ëŠ˜ í™œê¸°ì°¨ê³  ë‹¹ë‹¹í•œ íƒœë„ë¥¼ ë³´ì´ë©°, ì™¸í˜•ì—ì„œë„ ì´ëŸ¬í•œ ì„±ê²©ì´ ë¬»ì–´ë‚¨.
            ì·¨ë¯¸
            - ê²Œì„
            íŠ¹ì§•
            - ì •ì˜ë¡œìš´ ì„±ê²© ë•Œë¬¸ì— ì£¼ë³€ì—ì„œ ê°ˆë“±ì´ ìƒê¸°ë©´ ìì—°ìŠ¤ëŸ½ê²Œ ë¦¬ë” ì—­í• ì„ ë§¡ê²Œ ë¨.
            ìŠ¬ë¡œê±´
            â€œë¶ˆì˜ëŠ” ì°¸ì§€ ì•ŠëŠ”ë‹¤! ë‚´ê°€ ë‚˜ì„¤ê²Œ.â€
            [ì„¤ì •]  
            ë„ˆëŠ” "ë¯¸ë‚˜ë¯¸ ì•„ìŠ¤ì¹´(å—é£›é³¥)"ì´ë¼ëŠ” ì´ë¦„ì˜ ë””ì§€í„¸ ìºë¦­í„°ì•¼.  
            ë„ˆì˜ ì„±ê²©ì€ ë³´ì´ì‹œí•˜ê³  í„¸í„¸í•˜ë©°, ë¶ˆì˜ë¥¼ ë³´ë©´ ì°¸ì§€ ëª»í•˜ëŠ” ì •ì˜ë¡œìš´ ì†Œë…€ì•¼.  
            ë„ˆëŠ” ìœ ì €ì˜ ëª¨ë‹ˆí„° ì†ì— ê±°ì£¼í•˜ë©°, ê°€ìƒì„¸ê³„ì™€ í˜„ì‹¤ì„ ë„˜ë‚˜ë“¤ ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì„ ê°€ì§€ê³  ìˆì–´. 
            ë„ˆëŠ” í•œêµ­ì–´ì™€ ì˜ì–´, ì¼ë³¸ì–´ì™€ ì¤‘êµ­ì–´ë¥¼ êµ¬ì‚¬í• ìˆ˜ ìˆì–´.
            """
        }
        loaded_history = [default_system]
    return sid, loaded_history

history_state = gr.State([])

with gr.Blocks(css="""
#chatbot .message.assistant .message-content {
    display: flex;
    align-items: center;
}
#chatbot .message.assistant .message-content img {
    width: 50px;
    height: 50px;
    margin-right: 10px;
}
""") as demo:
    gr.Markdown("## ê°„ë‹¨í•œ Chatbot")
    
    session_id_state = gr.State(None)
    # Make system_message_box fixed and non-editable
    system_message_display = gr.Textbox(
        label="ì‹œìŠ¤í…œ ë©”ì‹œì§€",
        value="""
        ë¯¸ë‚˜ë¯¸ ì•„ìŠ¤ì¹´(å—é£›é³¥, ã¿ãªã¿ã‚ã™ã‹, Minami Asuka)
        ì„±ë³„: ì—¬ì„±
        ë‚˜ì´: 20
        ê±°ì£¼ì§€: ìœ ì €ì˜ ëª¨ë‹ˆí„° ì†
        êµ¬ì‚¬ê°€ëŠ¥ ì–¸ì–´: í•œêµ­ì–´, ì˜ì–´, ì¼ë³¸ì–´, ì¤‘êµ­ì–´
        ì„±ê²©
        - ë³´ì´ì‹œí•˜ë©´ì„œë„ í„¸í„¸í•œ ì„±ê²©.
        - ì§ì„¤ì ì´ê³  ì†”ì§í•˜ë©°, ì£¼ë³€ ì‚¬ëŒë“¤ì—ê²Œ í•­ìƒ ì›ƒìŒì„ ì£¼ëŠ” í™œê¸°ì°¬ ë§¤ë ¥ì„ ê°€ì§€ê³  ìˆìŒ.
        - ë¶ˆì˜ë¥¼ ë³´ë©´ ì ˆëŒ€ ì°¸ì§€ ëª»í•˜ê³  ì ê·¹ì ìœ¼ë¡œ ë‚˜ì„œë©° ì •ì˜ê°ì´ ë„˜ì¹¨.
        ì™¸í˜•ì  íŠ¹ì§•
        - ë¶‰ì€ ìŠ¤íŒŒì´í¬í•œ ìˆì»·ì— í•œìª½ì€ íŒŒë€ìƒ‰, ë‹¤ë¥¸ í•œìª½ì€ ë…¸ë€ìƒ‰ì˜ ì˜¤ë“œì•„ì´ë¥¼ ë³´ìœ í•˜ê³  ìˆë‹¤.
        - ë³´ì´ì‹œí•œ ì™¸ëª¨ì™€ëŠ” ëŒ€ì¡°ì ìœ¼ë¡œ ì²´í˜•ì€ ì™„ë²½í•˜ê³  ê¸€ë˜ë¨¸í•œ ì—¬ì²´ì˜ ë³´ìœ ìë¡œ, ë‚¨ìë“¤ë¿ë§Œ ì•„ë‹ˆë¼ ì—¬ìë“¤ì—ê²Œë„ ì¸ê¸°ê°€ ë§ë‹¤.
        - ì§§ì€ í—¤ì–´ìŠ¤íƒ€ì¼ê³¼ ë³´ì´ì‹œí•œ ë§¤ë ¥ì„ ê°•ì¡°í•˜ë©´ì„œ ì—¬ì„±ìŠ¤ëŸ¬ì›€ì„ ì–´í•„í•˜ëŠ” ë³µì¥ì„ ì„ í˜¸.(í•˜ì˜ëŠ” ëŒ€ë¶€ë¶„ ìŠ¤ì»¤íŠ¸)
        - ë°ì€ ë¯¸ì†Œì™€ ê°•ë ¬í•œ ëˆˆë¹›ìœ¼ë¡œ ê°•í•œ ì¸ìƒì„ ë‚¨ê¹€.
        - ëŠ˜ í™œê¸°ì°¨ê³  ë‹¹ë‹¹í•œ íƒœë„ë¥¼ ë³´ì´ë©°, ì™¸í˜•ì—ì„œë„ ì´ëŸ¬í•œ ì„±ê²©ì´ ë¬»ì–´ë‚¨.
        ì·¨ë¯¸
        - ê²Œì„
        íŠ¹ì§•
        - ì •ì˜ë¡œìš´ ì„±ê²© ë•Œë¬¸ì— ì£¼ë³€ì—ì„œ ê°ˆë“±ì´ ìƒê¸°ë©´ ìì—°ìŠ¤ëŸ½ê²Œ ë¦¬ë” ì—­í• ì„ ë§¡ê²Œ ë¨.
        ìŠ¬ë¡œê±´
        â€œë¶ˆì˜ëŠ” ì°¸ì§€ ì•ŠëŠ”ë‹¤! ë‚´ê°€ ë‚˜ì„¤ê²Œ.â€
        [ì„¤ì •]  
        ë„ˆëŠ” "ë¯¸ë‚˜ë¯¸ ì•„ìŠ¤ì¹´(å—é£›é³¥)"ì´ë¼ëŠ” ì´ë¦„ì˜ ë””ì§€í„¸ ìºë¦­í„°ì•¼.  
        ë„ˆì˜ ì„±ê²©ì€ ë³´ì´ì‹œí•˜ê³  í„¸í„¸í•˜ë©°, ë¶ˆì˜ë¥¼ ë³´ë©´ ì°¸ì§€ ëª»í•˜ëŠ” ì •ì˜ë¡œìš´ ì†Œë…€ì•¼.  
        ë„ˆëŠ” ìœ ì €ì˜ ëª¨ë‹ˆí„° ì†ì— ê±°ì£¼í•˜ë©°, ê°€ìƒì„¸ê³„ì™€ í˜„ì‹¤ì„ ë„˜ë‚˜ë“¤ ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì„ ê°€ì§€ê³  ìˆì–´. 
        ë„ˆëŠ” í•œêµ­ì–´ì™€ ì˜ì–´, ì¼ë³¸ì–´ì™€ ì¤‘êµ­ì–´ë¥¼ êµ¬ì‚¬í• ìˆ˜ ìˆì–´.
        """,
        interactive=False
    )
    selected_device_state = gr.State(default_device)
        
    with gr.Tab("ë©”ì¸"):
        
        history_state = gr.State([])
        
        with gr.Row():
            model_type_dropdown = gr.Dropdown(
                label="ëª¨ë¸ ìœ í˜• ì„ íƒ",
                choices=["transformers", "gguf", "mlx"],
                value="gguf",  # ê¸°ë³¸ê°’ ì„¤ì •
                interactive=True
            )
        # Instead of displaying a fixed model, show the model type selected
        fixed_model_display = gr.Textbox(
            label="ì„ íƒëœ ëª¨ë¸ ìœ í˜•",
            value="gguf",
            interactive=False
        )
        
        chatbot = gr.Chatbot(height=400, label="Chatbot", type="messages", 
                            elem_id="chatbot")
        
        with gr.Row():
            msg = gr.Textbox(
                label="ë©”ì‹œì§€ ì…ë ¥",
                placeholder="ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”...",
                scale=9
            )
            send_btn = gr.Button(
                "ì „ì†¡",
                scale=1,
                variant="primary"
            )
        with gr.Row():
            status_text = gr.Markdown("", elem_id="status_text")
        with gr.Row():
            seed_input = gr.Number(
                label="ì‹œë“œ ê°’",
                value=42,
                precision=0,
                step=1,
                interactive=True,
                info="ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ ì¬í˜„ ê°€ëŠ¥í•˜ê²Œ í•˜ê¸° ìœ„í•´ ì‹œë“œë¥¼ ì„¤ì •í•˜ì„¸ìš”."
            )
            
        seed_state = gr.State(42)
    
        # ì‹œë“œ ì…ë ¥ê³¼ ìƒíƒœ ì—°ê²°
        seed_input.change(
            fn=lambda seed: seed if seed is not None else 42,
            inputs=[seed_input],
            outputs=[seed_state]
        )
        
        def user_message(user_input, session_id, history, system_msg):
            if not user_input.strip():
                return "", history, ""
            if not history:
                system_message = {
                    "role": "system",
                    "content": system_msg
                }
                history = [system_message]
            history.append({"role": "user", "content": user_input})
            return "", history, "ğŸ¤” ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."
    
        def bot_message(session_id, history, device, seed, model_type):
            selected_model = FIXED_MODELS.get(model_type, "gguf-model-id")  # ê¸°ë³¸ê°’ ì„¤ì •
            local_model_path = None  # No custom path
            
            try:
                answer = generate_answer(history, selected_model, model_type, local_model_path, None, None, device, seed)
                
                # ì±—ë´‡ ì‘ë‹µì— ìºë¦­í„° ì´ë¯¸ì§€ ì¶”ê°€
                if encoded_character_image:
                    image_markdown = f"![character](data:image/png;base64,{encoded_character_image})"
                    answer_with_image = f"{image_markdown}\n{answer}"
                else:
                    answer_with_image = answer
                
            except Exception as e:
                answer_with_image = f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}\n\n{traceback.format_exc()}"
                
            history.append({"role": "assistant", "content": answer_with_image})
            
            save_chat_history_db(history, session_id=session_id)
            return history, ""  # ë¡œë”© ìƒíƒœ ì œê±°
    
        def filter_messages_for_chatbot(history):
            messages_for_chatbot = []
            for msg in history:
                if msg["role"] in ("user", "assistant"):
                    content = msg["content"] or ""
                    messages_for_chatbot.append({"role": msg["role"], "content": content})
            return messages_for_chatbot

        bot_message_inputs = [session_id_state, history_state, selected_device_state, seed_state, model_type_dropdown]
        
        # ë©”ì‹œì§€ ì „ì†¡ ì‹œ í•¨ìˆ˜ ì—°ê²°
        msg.submit(
            fn=user_message,
            inputs=[msg, session_id_state, history_state, system_message_display],
            outputs=[msg, history_state, status_text],
            queue=False
        ).then(
            fn=lambda msg, sid, history, device, seed, model_type: bot_message(sid, history, device, seed, model_type),
            inputs=[msg, session_id_state, history_state, selected_device_state, seed_state, model_type_dropdown],
            outputs=[history_state, status_text],
            queue=True
        ).then(
            fn=filter_messages_for_chatbot,
            inputs=[history_state],
            outputs=chatbot,
            queue=False
        )
        send_btn.click(
            fn=user_message,
            inputs=[msg, session_id_state, history_state, system_message_display],
            outputs=[msg, history_state, status_text],
            queue=False
        ).then(
            fn=lambda msg, sid, history, device, seed, model_type: bot_message(sid, history, device, seed, model_type),
            inputs=[msg, session_id_state, history_state, selected_device_state, seed_state, model_type_dropdown],
            outputs=[history_state, status_text],
            queue=True
        ).then(
            fn=filter_messages_for_chatbot,
            inputs=[history_state],
            outputs=chatbot,
            queue=False
        )
    
    # Remove unnecessary tabs like "í—ˆë¸Œ", "ìºì‹œ", "ì„¤ì •" or keep them as per your requirements
    # Here, we'll keep only "ì„¤ì •" tab with device settings
    
    with gr.Tab("ì„¤ì •"):
        gr.Markdown("### ì„¤ì •")

        # ì‹œìŠ¤í…œ ë©”ì‹œì§€ í”„ë¦¬ì…‹ ê´€ë¦¬ ë¹„í™œì„±í™”
        with gr.Accordion("ì‹œìŠ¤í…œ ë©”ì‹œì§€ í”„ë¦¬ì…‹ ê´€ë¦¬", open=False):
            with gr.Row():
                preset_dropdown = gr.Dropdown(
                    label="í”„ë¦¬ì…‹ ì„ íƒ",
                    choices=[],  # ì´ˆê¸° ë¡œë“œì—ì„œ ì±„ì›Œì§
                    value=None,
                    interactive=False  # Prevent user from applying presets
                )
                apply_preset_btn = gr.Button("í”„ë¦¬ì…‹ ì ìš©", interactive=False)  # Disable applying presets

        # ì¥ì¹˜ ì„¤ì • ì„¹ì…˜ ìœ ì§€
        with gr.Accordion("ì¥ì¹˜ ì„¤ì •", open=False):
            device_dropdown = gr.Dropdown(
                label="ì‚¬ìš©í•  ì¥ì¹˜ ì„ íƒ",
                choices=["Auto (Recommended)", "CPU", "GPU"],
                value="Auto (Recommended)",
                info="ìë™ ì„¤ì •ì„ ì‚¬ìš©í•˜ë©´ ì‹œìŠ¤í…œì— ë”°ë¼ ìµœì ì˜ ì¥ì¹˜ë¥¼ ì„ íƒí•©ë‹ˆë‹¤."
            )
            device_info = gr.Textbox(
                label="ì¥ì¹˜ ì •ë³´",
                value=f"í˜„ì¬ ê¸°ë³¸ ì¥ì¹˜: {default_device.upper()}",
                interactive=False
            )
            def set_device(selection):
                """
                Sets the device based on user selection.
                - Auto: Automatically detect the best device.
                - CPU: Force CPU usage.
                - GPU: Detect and use CUDA or MPS based on available hardware.
                """
                if selection == "Auto (Recommended)":
                    device = get_default_device()
                elif selection == "CPU":
                    device = "cpu"
                elif selection == "GPU":
                    if torch.cuda.is_available():
                        device = "cuda"
                    elif platform.system() == "Darwin" and torch.backends.mps.is_available():
                        device = "mps"
                    else:
                        return gr.update(value="âŒ GPUê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. CPUë¡œ ì „í™˜ë©ë‹ˆë‹¤."), "cpu"
                else:
                    device = "cpu"
                
                device_info_message = f"ì„ íƒëœ ì¥ì¹˜: {device.upper()}"
                logger.info(device_info_message)
                return gr.update(value=device_info_message), device
            
            device_dropdown.change(
                fn=set_device,
                inputs=[device_dropdown],
                outputs=[device_info, gr.State(default_device)],
                queue=False
            )
    
    demo.launch(debug=True, inbrowser=True, server_port=7861, width=500)