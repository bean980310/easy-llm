# app.py

import os
import shutil
import traceback
import torch
import gradio as gr
from transformers import AutoTokenizer, AutoModelForCausalLM
import openai
import logging
from logging.handlers import RotatingFileHandler
from model_handlers import (
    GGUFModelHandler,MiniCPMLlama3V25Handler, GLM4Handler, GLM4VHandler, VisionModelHandler,
    Aya23Handler, GLM4HfHandler, OtherModelHandler, QwenHandler, MlxModelHandler, MlxVisionHandler, MlxQwenHandler
)
from huggingface_hub import HfApi, list_models
from utils import (
    make_local_dir_name,
    get_all_local_models,  # ìˆ˜ì •ëœ í•¨ìˆ˜
    scan_local_models,
    get_model_list_from_hf_hub,
    download_model_from_hf,
    ensure_model_available,
    convert_and_save,
    
)
from cache import models_cache 


##########################################
# 1) ìœ í‹¸ í•¨ìˆ˜ë“¤
##########################################

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ë¡œê·¸ í¬ë§· ì •ì˜
formatter = logging.Formatter(
    fmt='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

# ì½˜ì†” í•¸ë“¤ëŸ¬ ì¶”ê°€
console_handler = logging.StreamHandler()
console_handler.setFormatter(formatter)
logger.addHandler(console_handler)

# íŒŒì¼ í•¸ë“¤ëŸ¬ ì¶”ê°€ (ë¡œí…Œì´íŒ… íŒŒì¼ í•¸ë“¤ëŸ¬ ì‚¬ìš©)
log_file = "app.log"  # ì›í•˜ëŠ” ë¡œê·¸ íŒŒì¼ ê²½ë¡œë¡œ ë³€ê²½ ê°€ëŠ¥
rotating_file_handler = RotatingFileHandler(
    log_file, maxBytes=5*1024*1024, backupCount=5  # 5MBë§ˆë‹¤ ìƒˆë¡œìš´ íŒŒì¼ë¡œ êµì²´, ìµœëŒ€ 5ê°œ ë°±ì—…
)
rotating_file_handler.setFormatter(formatter)
logger.addHandler(rotating_file_handler)

# ë©”ëª¨ë¦¬ ìƒì— ë¡œë“œëœ ëª¨ë¸ë“¤ì„ ì €ìž¥í•˜ëŠ” ìºì‹œ
LOCAL_MODELS_ROOT = "./models"

def build_model_cache_key(model_id: str, model_type: str, local_path: str = None) -> str:
    """
    models_cacheì— ì‚¬ìš©ë  keyë¥¼ êµ¬ì„±.
    - ë§Œì•½ model_id == 'Local (Custom Path)' ì´ê³  local_pathê°€ ì£¼ì–´ì§€ë©´ 'local::{local_path}'
    - ê·¸ ì™¸ì—ëŠ” 'auto::{model_type}::{local_dir}::hf::{model_id}' í˜•íƒœ.
    """
    if model_id == "Local (Custom Path)" and local_path:
        return f"local::{local_path}"
    elif model_type == "api":
        return f"api::{model_id}"
    else:
        local_dirname = make_local_dir_name(model_id)
        local_dirpath = os.path.join("./models", model_type, local_dirname)
        return f"auto::{model_type}::{local_dirpath}::hf::{model_id}"

def clear_model_cache(model_id: str, local_path: str = None) -> str:
    """
    íŠ¹ì • ëª¨ë¸ì— ëŒ€í•œ ìºì‹œë¥¼ ì œê±° (models_cacheì—ì„œ í•´ë‹¹ keyë¥¼ ì‚­ì œ).
    - ë§Œì•½ í•´ë‹¹ keyê°€ ì—†ìœ¼ë©´ 'ì´ë¯¸ ì—†ìŒ' ë©”ì‹œì§€ ë°˜í™˜
    - ì„±ê³µ ì‹œ 'ìºì‹œ ì‚­ì œ ì™„ë£Œ' ë©”ì‹œì§€
    """
    # ëª¨ë¸ ìœ í˜•ì„ ê²°ì •í•´ì•¼ í•©ë‹ˆë‹¤.
    if model_id in ["gpt-3.5-turbo", "gpt-4o", "gpt-4o-mini"]:
        model_type = "api"
    else:
        # ë¡œì»¬ ëª¨ë¸ì˜ ê¸°ë³¸ ìœ í˜•ì„ transformersë¡œ ì„¤ì • (í•„ìš” ì‹œ ìˆ˜ì •)
        model_type = "transformers"
    key = build_model_cache_key(model_id, model_type, local_path)
    if key in models_cache:
        del models_cache[key]
        msg = f"[cache] ëª¨ë¸ ìºì‹œ ì œê±°: {key}"
        logger.info(msg)
        return msg
    else:
        msg = f"[cache] ì´ë¯¸ ìºì‹œì— ì—†ê±°ë‚˜, ë¡œë“œëœ ì  ì—†ìŒ: {key}"
        logger.info(msg)
        return msg

def refresh_model_list():
    new_local_models = get_all_local_models()
    api_models = ["gpt-3.5-turbo", "gpt-4o-mini", "gpt-4o"]
    local_models = (
        new_local_models["transformers"] + 
        new_local_models["gguf"] + 
        new_local_models["mlx"]
    )
    new_choices = api_models + local_models
    new_choices = sorted(list(dict.fromkeys(new_choices)))
    return gr.update(choices=new_choices), "ëª¨ë¸ ëª©ë¡ì„ ìƒˆë¡œê³ ì¹¨í–ˆìŠµë‹ˆë‹¤."


def clear_all_model_cache():
    """
    í˜„ìž¬ ë©”ëª¨ë¦¬ì— ë¡œë“œëœ ëª¨ë“  ëª¨ë¸ ìºì‹œ(models_cache)ë¥¼ í•œ ë²ˆì— ì‚­ì œ.
    í•„ìš”í•˜ë‹¤ë©´, ë¡œì»¬ í´ë”ì˜ .cacheë“¤ë„ ì¼ê´„ ì‚­ì œí•  ìˆ˜ ìžˆìŒ.
    """
    # 1) ë©”ëª¨ë¦¬ ìºì‹œ ì „ë¶€ ì‚­ì œ
    count = len(models_cache)
    models_cache.clear()
    logger.info(f"[*] ë©”ëª¨ë¦¬ ìºì‹œ ì‚­ì œ: {count}ê°œ ëª¨ë¸")

    # 2) (ì„ íƒ) ë¡œì»¬ í´ë” .cache ì‚­ì œ
    #    ì˜ˆ: ./models/*/.cache í´ë” ì „ë¶€ ì‚­ì œ
    #    ì›ì¹˜ ì•Šìœ¼ë©´ ì£¼ì„ì²˜ë¦¬
    cache_deleted = 0
    for subdir, models in get_all_local_models().items():
        for folder in models:
            folder_path = os.path.join(LOCAL_MODELS_ROOT, subdir, folder)
            if os.path.isdir(folder_path):
                cache_path = os.path.join(folder_path, ".cache")
                if os.path.isdir(cache_path):
                    shutil.rmtree(cache_path)
                    cache_deleted += 1
    logger.info(f"[*] ë¡œì»¬ í´ë” .cache ì‚­ì œ: {cache_deleted}ê°œ í´ë” ì‚­ì œ")
    return f"[cache all] {count}ê°œ ëª¨ë¸ ìºì‹œ ì‚­ì œ ì™„ë£Œ. ë¡œì»¬ í´ë” .cache {cache_deleted}ê°œ ì‚­ì œ."

##########################################
# 2) ëª¨ë¸ ë¡œë“œ & ì¶”ë¡  ë¡œì§
##########################################

def load_model(selected_model, model_type, quantization_bit="Q8_0", local_model_path=None, api_key=None):
    """
    ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜. íŠ¹ì • ëª¨ë¸ì— ëŒ€í•œ ë¡œë“œ ë¡œì§ì„ ì™¸ë¶€ í•¸ë“¤ëŸ¬ë¡œ ë¶„ë¦¬.
    """
    model_id = selected_model
    if model_type not in ["transformers", "gguf", "mlx", "api"]:
        logger.error(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ëª¨ë¸ ìœ í˜•: {model_type}")
        return None
    if model_type == "api":
        # API ëª¨ë¸ì€ ë³„ë„ì˜ ë¡œë“œê°€ í•„ìš” ì—†ìœ¼ë¯€ë¡œ í•¸ë“¤ëŸ¬ ìƒì„± ì•ˆí•¨
        return None
    if model_type == "gguf":
        # GGUF ëª¨ë¸ ë¡œë”© ë¡œì§
        if not ensure_model_available(model_id, local_model_path, model_type):
            logger.error(f"ëª¨ë¸ '{model_id}'ì„(ë¥¼) ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
        handler = GGUFModelHandler(
            model_id=model_id,
            quantization_bit=quantization_bit,
            local_model_path=local_model_path,
            model_type=model_type
        )
        models_cache[build_model_cache_key(model_id, model_type, quantization_bit, local_model_path)] = handler
        return handler
    elif model_type == "mlx":
        # MLX ëª¨ë¸ ë¡œë”© ë¡œì§
        if model_id in ["Qwen/Qwen2-7B-Instruct-MLX", "mlx-community/Qwen2.5-7B-Instruct-4bit"]:
            if not ensure_model_available(model_id, local_model_path, model_type):
                logger.error(f"ëª¨ë¸ '{model_id}'ì„(ë¥¼) ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
            handler = MlxQwenHandler(
                model_id=model_id,
                local_model_path=local_model_path,
                model_type=model_type
            )
            models_cache[build_model_cache_key(model_id, model_type)] = handler
            return handler
        elif "vision" in model_id.lower():
            if not ensure_model_available(model_id, local_model_path, model_type):
                logger.error(f"ëª¨ë¸ '{model_id}'ì„(ë¥¼) ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
            handler = MlxVisionHandler(
                model_id=model_id,
                local_model_path=local_model_path,
                model_type=model_type
            )
            models_cache[build_model_cache_key(model_id, model_type)] = handler
            return handler
        else:
            if not ensure_model_available(model_id, local_model_path, model_type):
                logger.error(f"ëª¨ë¸ '{model_id}'ì„(ë¥¼) ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
            handler = MlxModelHandler(
                model_id=model_id,
                local_model_path=local_model_path,
                model_type=model_type
            )
            models_cache[build_model_cache_key(model_id, model_type)] = handler
            return handler
    else:
        if model_id == "openbmb/MiniCPM-Llama3-V-2_5":
            # ëª¨ë¸ ì¡´ìž¬ í™•ì¸ ë° ë‹¤ìš´ë¡œë“œ
            if not ensure_model_available(model_id, local_model_path, model_type):
                logger.error(f"ëª¨ë¸ '{model_id}'ì„(ë¥¼) ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
            handler = MiniCPMLlama3V25Handler(
                model_id=model_id,
                local_model_path=local_model_path,
                model_type=model_type
            )
            models_cache[build_model_cache_key(model_id, model_type)] = handler
            return handler
        elif model_id in [
            "Bllossom/llama-3.2-Korean-Bllossom-AICA-5B",
        ] or ("vision" in model_id.lower() and model_id != "Bllossom/llama-3.1-Korean-Bllossom-Vision-8B"):
            # ëª¨ë¸ ì¡´ìž¬ í™•ì¸ ë° ë‹¤ìš´ë¡œë“œ
            if not ensure_model_available(model_id, local_model_path, model_type):
                logger.error(f"ëª¨ë¸ '{model_id}'ì„(ë¥¼) ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
            handler = VisionModelHandler(
                model_id=model_id,
                local_model_path=local_model_path,
                model_type=model_type
            )
            models_cache[build_model_cache_key(model_id, model_type)] = handler
            return handler
        elif model_id == "THUDM/glm-4v-9b":
            # ëª¨ë¸ ì¡´ìž¬ í™•ì¸ ë° ë‹¤ìš´ë¡œë“œ
            if not ensure_model_available(model_id, local_model_path, model_type):
                logger.error(f"ëª¨ë¸ '{model_id}'ì„(ë¥¼) ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
            handler = GLM4VHandler(
                model_id=model_id,
                local_model_path=local_model_path,
                model_type=model_type
            )
            models_cache[build_model_cache_key(model_id, model_type)] = handler
            return handler
        elif model_id == "THUDM/glm-4-9b-chat":
            if not ensure_model_available(model_id, local_model_path, model_type):
                logger.error(f"ëª¨ë¸ '{model_id}'ì„(ë¥¼) ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
            handler = GLM4Handler(
                model_id=model_id,
                local_model_path=local_model_path,
                model_type=model_type
            )
            models_cache[build_model_cache_key(model_id, model_type)] = handler
            return handler
        elif model_id in ["THUDM/glm-4-9b-chat-hf", "THUDM/glm-4-9b-chat-1m-hf"]:
            if not ensure_model_available(model_id, local_model_path, model_type):
                logger.error(f"ëª¨ë¸ '{model_id}'ì„(ë¥¼) ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
            handler = GLM4HfHandler(
                model_id=model_id,  # model_idê°€ ì •ì˜ë˜ì–´ ìžˆì–´ì•¼ í•©ë‹ˆë‹¤.
                local_model_path=local_model_path,
                model_type=model_type
            )
            models_cache[build_model_cache_key(model_id, model_type)] = handler
            return handler
        elif model_id in ["bean980310/glm-4-9b-chat-hf_float8", "genai-archive/glm-4-9b-chat-hf_int8"]:
            # 'fp8' íŠ¹í™” í•¸ë“¤ëŸ¬ ë¡œì§ ì¶”ê°€
            if not ensure_model_available(model_id, local_model_path, model_type):
                logger.error(f"ëª¨ë¸ '{model_id}'ì„(ë¥¼) ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
            handler = GLM4HfHandler(
                model_id=model_id,  # model_idê°€ ì •ì˜ë˜ì–´ ìžˆì–´ì•¼ í•©ë‹ˆë‹¤.
                local_model_path=local_model_path,
                model_type=model_type
            )
            models_cache[build_model_cache_key(model_id, model_type)] = handler
            return handler
        elif model_id in ["CohereForAI/aya-23-8B", "CohereForAI/aya-23-35B"]:
            if not ensure_model_available(model_id, local_model_path, model_type):
                logger.error(f"ëª¨ë¸ '{model_id}'ì„(ë¥¼) ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
            handler = Aya23Handler(
                model_id=model_id,  # model_idê°€ ì •ì˜ë˜ì–´ ìžˆì–´ì•¼ í•©ë‹ˆë‹¤.
                local_model_path=local_model_path,
                model_type=model_type
            )
            models_cache[build_model_cache_key(model_id, model_type)] = handler
            return handler
        elif "qwen" in model_id.lower():
            if not ensure_model_available(model_id, local_model_path, model_type):
                logger.error(f"ëª¨ë¸ '{model_id}'ì„(ë¥¼) ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
            handler = QwenHandler(
                model_id=model_id,  # model_idê°€ ì •ì˜ë˜ì–´ ìžˆì–´ì•¼ í•©ë‹ˆë‹¤.
                local_model_path=local_model_path,
                model_type=model_type
            )
            models_cache[build_model_cache_key(model_id, model_type)] = handler
            return handler
        else:
            if not ensure_model_available(model_id, local_model_path, model_type):
                logger.error(f"ëª¨ë¸ '{model_id}'ì„(ë¥¼) ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
            handler = OtherModelHandler(model_id, local_model_path=local_model_path, model_type=model_type)
            models_cache[build_model_cache_key(model_id, model_type)] = handler
            return handler

def generate_answer(history, selected_model, model_type, local_model_path=None, image_input=None, api_key=None):
    """
    ì‚¬ìš©ìž ížˆìŠ¤í† ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ ìƒì„±.
    """
    if not history:
        system_message = {
            "role": "system",
            "content": "ë‹¹ì‹ ì€ ìœ ìš©í•œ AI ë¹„ì„œìž…ë‹ˆë‹¤."
        }
        history = [system_message]
    
    cache_key = build_model_cache_key(selected_model, model_type, local_path=local_model_path)
    handler = models_cache.get(cache_key)
    
    if model_type == "api":
        if not api_key:
            logger.error("OpenAI API Keyê°€ missing.")
            return "OpenAI API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤."
        openai.api_key = api_key
        messages = [{"role": msg['role'], "content": msg['content']} for msg in history]
        logger.info(f"[*] OpenAI API ìš”ì²­: {messages}")
        
        try:
            response = openai.ChatCompletion.create(
                model=selected_model,
                messages=messages,
                temperature=0.7,
                max_tokens=1024,
                top_p=0.9
            )
            answer = response.choices[0].message["content"]
            logger.info(f"[*] OpenAI ì‘ë‹µ: {answer}")
            return answer
        except Exception as e:
            logger.error(f"OpenAI API ì˜¤ë¥˜: {str(e)}\n\n{traceback.format_exc()}")
            return f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}\n\n{traceback.format_exc()}"
    
    else:
        if not handler:
            logger.info(f"[*] ëª¨ë¸ ë¡œë“œ ì¤‘: {selected_model}")
            handler = load_model(selected_model, model_type, local_model_path=local_model_path)
        
        if not handler:
            logger.error("ëª¨ë¸ í•¸ë“¤ëŸ¬ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return "ëª¨ë¸ í•¸ë“¤ëŸ¬ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        logger.info(f"[*] Generating answer using {handler.__class__.__name__}")
        try:
            if isinstance(handler, VisionModelHandler):
                answer = handler.generate_answer(history, image_input)
            else:
                answer = handler.generate_answer(history)
            return answer
        except Exception as e:
            logger.error(f"ëª¨ë¸ ì¶”ë¡  ì˜¤ë¥˜: {str(e)}\n\n{traceback.format_exc()}")
            return f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}\n\n{traceback.format_exc()}"

##########################################
# 3) Gradio UI
##########################################

with gr.Blocks() as demo:
    gr.Markdown("## ê°„ë‹¨í•œ Chatbot")
    api_models = [
        "gpt-3.5-turbo",
        "gpt-4o-mini",
        "gpt-4o"
        # í•„ìš” ì‹œ ì¶”ê°€
    ]
    
    # HuggingFaceì—ì„œ ì§€ì›í•˜ëŠ” ê¸°ë³¸ ëª¨ë¸ ëª©ë¡
    known_hf_models = [
        "meta-llama/Llama-3.1-8B",
        "meta-llama/Llama-3.2-11B-Vision",
        "meta-llama/Llama-3.2-11B-Vision-Instruct",
        "openbmb/MiniCPM-Llama3-V-2_5",
        "Bllossom/llama-3.2-Korean-Bllossom-3B",
        "Bllossom/llama-3.2-Korean-Bllossom-AICA-5B",
        "Bllossom/llama-3.1-Korean-Bllossom-Vision-8B",
        "THUDM/glm-4-9b-chat",
        "THUDM/glm-4-9b-chat-hf",
        "THUDM/glm-4-9b-chat-1m",
        "THUDM/glm-4-9b-chat-1m-hf",
        "THUDM/glm-4v-9b",
        "huggyllama/llama-7b",
        "OrionStarAI/Orion-14B-Base",
        "OrionStarAI/Orion-14B-Chat",
        "OrionStarAI/Orion-14B-LongChat",
        "CohereForAI/aya-23-8B",
        "CohereForAI/aya-23-35B",
        "Qwen/Qwen2.5-7B",
        "Qwen/Qwen2.5-7B-Instruct",
        "Qwen/Qwen2.5-14B",
        "Qwen/Qwen2.5-14B-Instruct",
        "EleutherAI/polyglot-ko-1.3b"
    ]
    
    local_models_data = get_all_local_models()
    transformers_local = local_models_data["transformers"]
    gguf_local = local_models_data["gguf"]
    mlx_local = local_models_data["mlx"]
    
    custom_model_path_state = gr.State("")
    
    system_message_box = gr.Textbox(
        label="ì‹œìŠ¤í…œ ë©”ì‹œì§€",
        value="ë‹¹ì‹ ì€ ìœ ìš©í•œ AI ë¹„ì„œìž…ë‹ˆë‹¤.",
        placeholder="ëŒ€í™”ì˜ ì„±ê²©, ë§íˆ¬ ë“±ì„ ì •ì˜í•˜ì„¸ìš”."
    )
        
    with gr.Tab("ë©”ì¸"):
        initial_choices = api_models + transformers_local + gguf_local + mlx_local + ["ì‚¬ìš©ìž ì§€ì • ëª¨ë¸ ê²½ë¡œ ë³€ê²½"]
        initial_choices = list(dict.fromkeys(initial_choices))
        initial_choices = sorted(initial_choices)  # ì •ë ¬ ì¶”ê°€
        
        with gr.Row():
            model_type_dropdown = gr.Radio(
                label="ëª¨ë¸ ìœ í˜• ì„ íƒ",
                choices=["all", "transformers", "gguf", "mlx"],
                value="all",
            )
        
        model_dropdown = gr.Dropdown(
            label="ëª¨ë¸ ì„ íƒ",
            choices=initial_choices,
            value=initial_choices[0] if len(initial_choices) > 0 else None,
        )
        
        api_key_text = gr.Textbox(
            label="OpenAI API Key",
            placeholder="sk-...",
            visible=False  # ê¸°ë³¸ì ìœ¼ë¡œ ìˆ¨ê¹€
        )
        image_info = gr.Markdown("", visible=False)
        with gr.Column():
            with gr.Row():
                image_input = gr.Image(label="ì´ë¯¸ì§€ ì—…ë¡œë“œ (ì„ íƒ)", type="pil", visible=False)
                chatbot = gr.Chatbot(height=400, label="Chatbot", type="messages")
            with gr.Row():
                msg = gr.Textbox(
                    label="ë©”ì‹œì§€ ìž…ë ¥",
                    placeholder="ë©”ì‹œì§€ë¥¼ ìž…ë ¥í•˜ì„¸ìš”...",
                    scale=9
                )
                send_btn = gr.Button(
                    "ì „ì†¡",
                    scale=1,
                    variant="primary"
                )
            with gr.Row():
                status_text = gr.Markdown("", elem_id="status_text")
        history_state = gr.State([])
        
        # í•¨ìˆ˜: OpenAI API Keyì™€ ì‚¬ìš©ìž ì§€ì • ëª¨ë¸ ê²½ë¡œ í•„ë“œì˜ ê°€ì‹œì„± ì œì–´
        def toggle_api_key_visibility(selected_model):
            """
            OpenAI API Key ìž…ë ¥ í•„ë“œì˜ ê°€ì‹œì„±ì„ ì œì–´í•©ë‹ˆë‹¤.
            """
            api_visible = selected_model in api_models
            return gr.update(visible=api_visible)

        def toggle_image_input_visibility(selected_model):
            """
            ì´ë¯¸ì§€ ìž…ë ¥ í•„ë“œì˜ ê°€ì‹œì„±ì„ ì œì–´í•©ë‹ˆë‹¤.
            """
            image_visible = (
                "vision" in selected_model.lower() or
                "Vision" in selected_model or
                selected_model in [
                    "Bllossom/llama-3.2-Korean-Bllossom-AICA-5B",
                    "THUDM/glm-4v-9b",
                    "openbmb/MiniCPM-Llama3-V-2_5"
                ]
            )
            return gr.update(visible=image_visible)
        
        # ëª¨ë¸ ì„ íƒ ë³€ê²½ ì‹œ ê°€ì‹œì„± í† ê¸€
        model_dropdown.change(
            fn=lambda selected_model: (
                toggle_api_key_visibility(selected_model),
                toggle_image_input_visibility(selected_model)
            ),
            inputs=[model_dropdown],
            outputs=[api_key_text, image_input]
        )
        def update_model_list(selected_type):
            local_models_data = get_all_local_models()
            transformers_local = local_models_data["transformers"]
            gguf_local = local_models_data["gguf"]
            mlx_local = local_models_data["mlx"]
            
            # "ì „ì²´ ëª©ë¡"ì´ë©´ => API ëª¨ë¸ + ëª¨ë“  ë¡œì»¬ ëª¨ë¸ + "ì‚¬ìš©ìž ì§€ì • ëª¨ë¸ ê²½ë¡œ ë³€ê²½"
            if selected_type == "all":
                all_models = api_models + transformers_local + gguf_local + mlx_local + ["ì‚¬ìš©ìž ì§€ì • ëª¨ë¸ ê²½ë¡œ ë³€ê²½"]
                # ì¤‘ë³µ ì œê±° í›„ ì •ë ¬
                all_models = sorted(list(dict.fromkeys(all_models)))
                return gr.update(choices=all_models, value=all_models[0] if all_models else None)
            
            # ê°œë³„ í•­ëª©ì´ë©´ => í•´ë‹¹ ìœ í˜•ì˜ ë¡œì»¬ ëª¨ë¸ + "ì‚¬ìš©ìž ì§€ì • ëª¨ë¸ ê²½ë¡œ ë³€ê²½"ë§Œ
            if selected_type == "transformers":
                updated_list = transformers_local + ["ì‚¬ìš©ìž ì§€ì • ëª¨ë¸ ê²½ë¡œ ë³€ê²½"]
            elif selected_type == "gguf":
                updated_list = gguf_local + ["ì‚¬ìš©ìž ì§€ì • ëª¨ë¸ ê²½ë¡œ ë³€ê²½"]
            elif selected_type == "mlx":
                updated_list = mlx_local + ["ì‚¬ìš©ìž ì§€ì • ëª¨ë¸ ê²½ë¡œ ë³€ê²½"]
            else:
                # í˜¹ì‹œ ì˜ˆìƒì¹˜ ëª»í•œ ê°’ì´ë©´ transformersë¡œ ì²˜ë¦¬(ë˜ëŠ” None)
                updated_list = transformers_local + ["ì‚¬ìš©ìž ì§€ì • ëª¨ë¸ ê²½ë¡œ ë³€ê²½"]
            
            updated_list = sorted(list(dict.fromkeys(updated_list)))
            return gr.update(choices=updated_list, value=updated_list[0] if updated_list else None)
        
        model_type_dropdown.change(
            fn=update_model_list,
            inputs=[model_type_dropdown],
            outputs=[model_dropdown]
        )
        
        def user_message(user_input, history, system_msg):
            if not user_input.strip():
                return "", history, ""
            if not history:
                system_message = {
                    "role": "system",
                    "content": system_msg
                }
                history = [system_message]
            history = history + [{"role": "user", "content": user_input}]
            return "", history, "ðŸ¤” ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ìž…ë‹ˆë‹¤..."
    
        def bot_message(history, selected_model, custom_path, image, api_key):
            # ëª¨ë¸ ìœ í˜• ê²°ì •
            local_model_path = None
            if selected_model in api_models:
                model_type = "api"
                local_model_path = None
            elif selected_model == "ì‚¬ìš©ìž ì§€ì • ëª¨ë¸ ê²½ë¡œ ë³€ê²½":
                # ì‚¬ìš©ìž ì§€ì • ëª¨ë¸ ê²½ë¡œ ì‚¬ìš©
                model_type = "transformers"  # ê¸°ë³¸ ëª¨ë¸ ìœ í˜• ì„¤ì •, í•„ìš” ì‹œ ìˆ˜ì •
                local_model_path = custom_path
            else:
                # ë¡œì»¬ ëª¨ë¸ ìœ í˜• ê²°ì • (transformers, gguf, mlx)
                if selected_model in transformers_local:
                    model_type = "transformers"
                elif selected_model in gguf_local:
                    model_type = "gguf"
                elif selected_model in mlx_local:
                    model_type = "mlx"
                else:
                    model_type = "transformers"  # ê¸°ë³¸ê°’
                local_model_path = None  # ê¸°ë³¸ ë¡œì»¬ ê²½ë¡œ ì‚¬ìš©
                
            try:
                answer = generate_answer(history, selected_model, model_type, local_model_path, image, api_key)
            except Exception as e:
                answer = f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}\n\n{traceback.format_exc()}"
            history = history + [{"role": "assistant", "content": answer}]
            return history, ""  # ë¡œë”© ìƒíƒœ ì œê±°
    

        def filter_messages_for_chatbot(history):
            """system ë©”ì‹œì§€ëŠ” ì œì™¸í•˜ê³  user/assistantë§Œ Chatbotìœ¼ë¡œ ë³´ëƒ„"""
            messages_for_chatbot = []
            for msg in history:
                if msg["role"] in ("user", "assistant"):
                    # contentê°€ Noneì´ë©´ ë¹ˆ ë¬¸ìžì—´ì´ë¼ë„ ë„£ì–´ì¤Œ
                    content = msg["content"] if msg["content"] is not None else ""
                    messages_for_chatbot.append({"role": msg["role"], "content": content})
            return messages_for_chatbot

        # ë©”ì‹œì§€ ì „ì†¡ ì‹œ í•¨ìˆ˜ ì—°ê²°
        msg.submit(
            fn=user_message,
            inputs=[msg, history_state, system_message_box],  # ì„¸ ë²ˆì§¸ íŒŒë¼ë¯¸í„° ì¶”ê°€
            outputs=[msg, history_state, status_text],
            queue=False
        ).then(
            fn=bot_message,
            inputs=[history_state, model_dropdown, custom_model_path_state, image_input, api_key_text],
            outputs=[history_state, status_text],
            queue=True
        ).then(
            fn=lambda h: h,
            inputs=history_state,
            outputs=chatbot,
            queue=False
        )
        send_btn.click(
            fn=user_message,
            inputs=[msg, history_state, system_message_box],
            outputs=[msg, history_state, status_text],
            queue=False
        ).then(
            fn=bot_message,
            inputs=[history_state, model_dropdown, custom_model_path_state, image_input, api_key_text],
            outputs=[history_state, status_text],
            queue=True
        ).then(
            fn=filter_messages_for_chatbot,            # ì¶”ê°€ëœ ë¶€ë¶„
            inputs=[history_state],
            outputs=chatbot,                           # chatbotì— ìµœì¢… ì „ë‹¬
            queue=False
        )
    
    with gr.Tab("ë‹¤ìš´ë¡œë“œ"):
        gr.Markdown("""### ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
        HuggingFaceì—ì„œ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  ë¡œì»¬ì— ì €ìž¥í•©ë‹ˆë‹¤. 
        ë¯¸ë¦¬ ì •ì˜ëœ ëª¨ë¸ ëª©ë¡ì—ì„œ ì„ íƒí•˜ê±°ë‚˜, ì»¤ìŠ¤í…€ ëª¨ë¸ IDë¥¼ ì§ì ‘ ìž…ë ¥í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.""")
        
        with gr.Column():
            # ë‹¤ìš´ë¡œë“œ ëª¨ë“œ ì„ íƒ (ë¼ë””ì˜¤ ë²„íŠ¼)
            download_mode = gr.Radio(
                label="ë‹¤ìš´ë¡œë“œ ë°©ì‹ ì„ íƒ",
                choices=["Predefined", "Custom Repo ID"],
                value="Predefined",
                container=True,
            )
            # ëª¨ë¸ ì„ íƒ/ìž…ë ¥ ì˜ì—­
            with gr.Column(visible=True) as predefined_column:
                predefined_dropdown = gr.Dropdown(
                    label="ëª¨ë¸ ì„ íƒ",
                    choices=sorted(known_hf_models),
                    value=known_hf_models[0] if known_hf_models else None,
                    info="ì§€ì›ë˜ëŠ” ëª¨ë¸ ëª©ë¡ìž…ë‹ˆë‹¤."
                )
                
            with gr.Column(visible=False) as custom_column:
                custom_repo_id_box = gr.Textbox(
                    label="Custom Model ID",
                    placeholder="ì˜ˆ) facebook/opt-350m",
                    info="HuggingFaceì˜ ëª¨ë¸ IDë¥¼ ìž…ë ¥í•˜ì„¸ìš” (ì˜ˆ: organization/model-name)"
                )
                
            # ë‹¤ìš´ë¡œë“œ ì„¤ì •
            with gr.Row():
                with gr.Column(scale=2):
                    target_path = gr.Textbox(
                        label="ì €ìž¥ ê²½ë¡œ",
                        placeholder="./models/my-model",
                        value="",
                        interactive=True,
                        info="ë¹„ì›Œë‘ë©´ ìžë™ìœ¼ë¡œ ê²½ë¡œê°€ ìƒì„±ë©ë‹ˆë‹¤."
                    )
                with gr.Column(scale=1):
                    use_auth = gr.Checkbox(
                        label="ì¸ì¦ í•„ìš”",
                        value=False,
                        info="ë¹„ê³µê°œ ë˜ëŠ” gated ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œ ì²´í¬"
                    )
            
            with gr.Column(visible=False) as auth_column:
                hf_token = gr.Textbox(
                    label="HuggingFace Token",
                    placeholder="hf_...",
                    type="password",
                    info="HuggingFaceì—ì„œ ë°œê¸‰ë°›ì€ í† í°ì„ ìž…ë ¥í•˜ì„¸ìš”."
                )
            
            # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ê³¼ ì§„í–‰ ìƒíƒœ
            with gr.Row():
                download_btn = gr.Button(
                    "ë‹¤ìš´ë¡œë“œ ì‹œìž‘",
                    variant="primary",
                    scale=2
                )
                cancel_btn = gr.Button(
                    "ì·¨ì†Œ",
                    variant="stop",
                    scale=1,
                    interactive=False
                )
                
            # ìƒíƒœ í‘œì‹œ
            download_status = gr.Markdown("")
            progress_bar = gr.Progress(
                track_tqdm=True,  # tqdm progress barsë¥¼ ì¶”ì 
            )
            
            # ë‹¤ìš´ë¡œë“œ ê²°ê³¼ì™€ ë¡œê·¸
            with gr.Accordion("ìƒì„¸ ì •ë³´", open=False):
                download_info = gr.TextArea(
                    label="ë‹¤ìš´ë¡œë“œ ë¡œê·¸",
                    interactive=False,
                    max_lines=10,
                    autoscroll=True
                )

        # UI ë™ìž‘ ì œì–´ë¥¼ ìœ„í•œ í•¨ìˆ˜ë“¤
        def toggle_download_mode(mode):
            """ë‹¤ìš´ë¡œë“œ ëª¨ë“œì— ë”°ë¼ UI ì»´í¬ë„ŒíŠ¸ í‘œì‹œ/ìˆ¨ê¹€"""
            return [
                gr.update(visible=(mode == "Predefined")),  # predefined_column
                gr.update(visible=(mode == "Custom Repo ID"))  # custom_column
            ]

        def toggle_auth(use_auth_val):
            """ì¸ì¦ í•„ìš” ì—¬ë¶€ì— ë”°ë¼ í† í° ìž…ë ¥ì°½ í‘œì‹œ/ìˆ¨ê¹€"""
            return gr.update(visible=use_auth_val)

        def download_with_progress(mode, predefined_choice, custom_repo, target_dir, use_auth_val, token):
            try:
                repo_id = predefined_choice if mode == "Predefined" else custom_repo.strip()
                if not repo_id:
                    yield (
                        "âŒ ëª¨ë¸ IDë¥¼ ìž…ë ¥í•´ì£¼ì„¸ìš”.",  # status
                        gr.update(interactive=True),  # download_btn
                        gr.update(interactive=False),  # cancel_btn
                        "ë‹¤ìš´ë¡œë“œê°€ ì‹œìž‘ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",  # download_info
                        gr.Dropdown.update()
                    )
                    return
                
                # ëª¨ë¸ ìœ í˜• ê²°ì •
                if "gguf" in repo_id.lower():
                    model_type = "gguf"
                elif "mlx" in repo_id.lower():
                    model_type = "mlx"
                else:
                    model_type = "transformers"

                # ì§„í–‰ ìƒíƒœ ì´ˆê¸°í™”
                yield (
                    "ðŸ”„ ë‹¤ìš´ë¡œë“œ ì¤€ë¹„ ì¤‘...",
                    gr.update(interactive=False),
                    gr.update(interactive=True),
                    f"ëª¨ë¸: {repo_id}\nì¤€ë¹„ ì¤‘...",
                    gr.Dropdown.update()
                )

                # ì‹¤ì œ ë‹¤ìš´ë¡œë“œ ìˆ˜í–‰
                yield (
                    "ðŸ”„ ë‹¤ìš´ë¡œë“œ ì¤‘...",
                    gr.update(interactive=False),
                    gr.update(interactive=True),
                    "ë‹¤ìš´ë¡œë“œë¥¼ ì§„í–‰ ì¤‘ìž…ë‹ˆë‹¤...",
                    gr.Dropdown.update()
                )
                result = download_model_from_hf(
                    repo_id,
                    target_dir or os.path.join("./models", model_type, make_local_dir_name(repo_id)),
                    model_type=model_type
                )

                # ë‹¤ìš´ë¡œë“œ ì™„ë£Œ í›„ UI ì—…ë°ì´íŠ¸
                yield (
                    "âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!" if "ì‹¤íŒ¨" not in result else "âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨",
                    gr.update(interactive=True),
                    gr.update(interactive=False),
                    result,
                    gr.Dropdown.update(choices=sorted(api_models + get_all_local_models()["transformers"] + get_all_local_models()["gguf"] + get_all_local_models()["mlx"] + ["ì‚¬ìš©ìž ì§€ì • ëª¨ë¸ ê²½ë¡œ ë³€ê²½"]))
                )

            except Exception as e:
                yield (
                    "âŒ ì˜¤ë¥˜ ë°œìƒ",
                    gr.update(interactive=True),
                    gr.update(interactive=False),
                    f"ì˜¤ë¥˜: {str(e)}\n\n{traceback.format_exc()}",
                    gr.Dropdown.update()
                )

        # ì´ë²¤íŠ¸ ì—°ê²°
        download_mode.change(
            fn=toggle_download_mode,
            inputs=download_mode,
            outputs=[predefined_column, custom_column]
        )
        
        use_auth.change(
            fn=toggle_auth,
            inputs=use_auth,
            outputs=[auth_column]
        )
        
        download_btn.click(
            fn=download_with_progress,
            inputs=[
                download_mode,
                predefined_dropdown,
                custom_repo_id_box,
                target_path,
                use_auth,
                hf_token
            ],
            outputs=[
                download_status,
                download_btn,
                cancel_btn,
                download_info,
                model_dropdown  # model_dropdownì„ ì—…ë°ì´íŠ¸í•˜ë„ë¡ ë³€ê²½
            ]
        )
    with gr.Tab("í—ˆë¸Œ"):
        gr.Markdown("""### í—ˆê¹…íŽ˜ì´ìŠ¤ í—ˆë¸Œ ëª¨ë¸ ê²€ìƒ‰
        í—ˆê¹…íŽ˜ì´ìŠ¤ í—ˆë¸Œì—ì„œ ëª¨ë¸ì„ ê²€ìƒ‰í•˜ê³  ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. 
        í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•˜ê±°ë‚˜ í•„í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì›í•˜ëŠ” ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.""")
        
        with gr.Row():
            search_box = gr.Textbox(
                label="ê²€ìƒ‰ì–´",
                placeholder="ëª¨ë¸ ì´ë¦„, íƒœê·¸ ë˜ëŠ” í‚¤ì›Œë“œë¥¼ ìž…ë ¥í•˜ì„¸ìš”",
                scale=4
            )
            search_btn = gr.Button("ê²€ìƒ‰", scale=1)
            
        with gr.Row():
            with gr.Column(scale=1):
                model_type_filter = gr.Dropdown(
                    label="ëª¨ë¸ ìœ í˜•",
                    choices=["All", "Text Generation", "Vision", "Audio", "Other"],
                    value="All"
                )
                language_filter = gr.Dropdown(
                    label="ì–¸ì–´",
                    choices=["All", "Korean", "English", "Chinese", "Japanese", "Multilingual"],
                    value="All"
                )
                library_filter = gr.Dropdown(
                    label="ë¼ì´ë¸ŒëŸ¬ë¦¬",
                    choices=["All", "Transformers", "GGUF", "MLX"],
                    value="All"
                )
            with gr.Column(scale=3):
                model_list = gr.Dataframe(
                    headers=["Model ID", "Description", "Downloads", "Likes"],
                    label="ê²€ìƒ‰ ê²°ê³¼",
                    interactive=False
                )
        
        with gr.Row():
            selected_model = gr.Textbox(
                label="ì„ íƒëœ ëª¨ë¸",
                interactive=False
            )
            
        # ë‹¤ìš´ë¡œë“œ ì„¤ì •
        with gr.Row():
            with gr.Column(scale=2):
                target_path = gr.Textbox(
                    label="ì €ìž¥ ê²½ë¡œ",
                    placeholder="./models/my-model",
                    value="",
                    interactive=True,
                    info="ë¹„ì›Œë‘ë©´ ìžë™ìœ¼ë¡œ ê²½ë¡œê°€ ìƒì„±ë©ë‹ˆë‹¤."
                )
            with gr.Column(scale=1):
                use_auth = gr.Checkbox(
                    label="ì¸ì¦ í•„ìš”",
                    value=False,
                    info="ë¹„ê³µê°œ ë˜ëŠ” gated ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œ ì²´í¬"
                )
        
        with gr.Column(visible=False) as auth_column:
            hf_token = gr.Textbox(
                label="HuggingFace Token",
                placeholder="hf_...",
                type="password",
                info="HuggingFaceì—ì„œ ë°œê¸‰ë°›ì€ í† í°ì„ ìž…ë ¥í•˜ì„¸ìš”."
            )
        
        # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ê³¼ ì§„í–‰ ìƒíƒœ
        with gr.Row():
            download_btn = gr.Button(
                "ë‹¤ìš´ë¡œë“œ",
                variant="primary",
                scale=2
            )
            cancel_btn = gr.Button(
                "ì·¨ì†Œ",
                variant="stop",
                scale=1,
                interactive=False
            )
            
        # ìƒíƒœ í‘œì‹œ
        download_status = gr.Markdown("")
        progress_bar = gr.Progress(track_tqdm=True)
        
        # ë‹¤ìš´ë¡œë“œ ê²°ê³¼ì™€ ë¡œê·¸
        with gr.Accordion("ìƒì„¸ ì •ë³´", open=False):
            download_info = gr.TextArea(
                label="ë‹¤ìš´ë¡œë“œ ë¡œê·¸",
                interactive=False,
                max_lines=10,
                autoscroll=True
            )

        def search_models(query, model_type, language, library):
            """í—ˆê¹…íŽ˜ì´ìŠ¤ í—ˆë¸Œì—ì„œ ëª¨ë¸ ê²€ìƒ‰"""
            try:
                api = HfApi()
                # ê²€ìƒ‰ í•„í„° êµ¬ì„±
                filter_str = ""
                if model_type != "All":
                    filter_str += f"task_{model_type.lower().replace(' ', '_')}"
                if language != "All":
                    if filter_str:
                        filter_str += " AND "
                    filter_str += f"language_{language.lower()}"
                if library != "All":
                    filter_str += f"library_{library.lower()}"
                
                # ëª¨ë¸ ê²€ìƒ‰ ìˆ˜í–‰
                models = api.list_models(
                    filter=filter_str if filter_str else None,
                    limit=100,
                    sort="lastModified",
                    direction=-1
                )
                
                filtered_models = [model for model in models if query.lower() in model.id.lower()]
                
                # ê²°ê³¼ ë°ì´í„°í”„ë ˆìž„ êµ¬ì„±
                model_list = []
                for model in filtered_models:
                    description = model.cardData.get('description', '') if model.cardData else 'No description available.'
                    short_description = (description[:100] + "...") if len(description) > 100 else description
                    model_list.append([
                        model.id,
                        short_description,
                        model.downloads,
                        model.likes
                    ])
                return model_list
            except Exception as e:
                logger.error(f"ëª¨ë¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\n\n{traceback.format_exc()}")
                return [["ì˜¤ë¥˜ ë°œìƒ", str(e), "", ""]]

        def select_model(evt: gr.SelectData, data):
            """ë°ì´í„°í”„ë ˆìž„ì—ì„œ ëª¨ë¸ ì„ íƒ"""
            selected_model_id = data.at[evt.index[0], "Model ID"]  # ì„ íƒëœ í–‰ì˜ 'Model ID' ì»¬ëŸ¼ ê°’
            return selected_model_id
        
        def toggle_auth(use_auth_val):
            """ì¸ì¦ í•„ìš” ì—¬ë¶€ì— ë”°ë¼ í† í° ìž…ë ¥ì°½ í‘œì‹œ/ìˆ¨ê¹€"""
            return gr.update(visible=use_auth_val)
        
        def download_model_with_progress(model_id, target_dir, use_auth_val, token, progress=gr.Progress()):
            """ì§„í–‰ë¥  í‘œì‹œì™€ í•¨ê»˜ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ìˆ˜í–‰"""
            try:
                if not model_id:
                    yield (
                        "âŒ ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”.",
                        gr.update(interactive=True),
                        gr.update(interactive=False),
                        "ë‹¤ìš´ë¡œë“œê°€ ì‹œìž‘ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
                        gr.Dropdown.update()
                    )
                    return
                
                # ëª¨ë¸ ìœ í˜• ê²°ì •
                model_type = "transformers"  # ê¸°ë³¸ê°’
                if "gguf" in model_id.lower():
                    model_type = "gguf"
                elif "mlx" in model_id.lower():
                    model_type = "mlx"

                # ì§„í–‰ ìƒíƒœ ì´ˆê¸°í™”
                progress(0, desc="ì¤€ë¹„ ì¤‘...")
                yield (
                    "ðŸ”„ ë‹¤ìš´ë¡œë“œ ì¤€ë¹„ ì¤‘...",
                    gr.update(interactive=False),
                    gr.update(interactive=True),
                    f"ëª¨ë¸: {model_id}\nì¤€ë¹„ ì¤‘...",
                    gr.Dropdown.update()
                )

                # ì‹¤ì œ ë‹¤ìš´ë¡œë“œ ìˆ˜í–‰
                progress(0.5, desc="ë‹¤ìš´ë¡œë“œ ì¤‘...")
                result = download_model_from_hf(
                    model_id,
                    target_dir or os.path.join("./models", model_type, make_local_dir_name(model_id)),
                    model_type=model_type,
                    token=token if use_auth_val else None
                )

                # ë‹¤ìš´ë¡œë“œ ì™„ë£Œ í›„ UI ì—…ë°ì´íŠ¸
                progress(1.0, desc="ì™„ë£Œ")
                local_models_data = get_all_local_models()
                local_models = (
                    local_models_data["transformers"] +
                    local_models_data["gguf"] +
                    local_models_data["mlx"]
                )
                new_choices = api_models + local_models + ["ì‚¬ìš©ìž ì§€ì • ëª¨ë¸ ê²½ë¡œ ë³€ê²½"]
                new_choices = list(dict.fromkeys(new_choices))
                new_choices = sorted(new_choices)

                yield (
                    "âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!" if "ì‹¤íŒ¨" not in result else "âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨",
                    gr.update(interactive=True),
                    gr.update(interactive=False),
                    result,
                    gr.Dropdown.update(choices=new_choices)
                )

            except Exception as e:
                yield (
                    "âŒ ì˜¤ë¥˜ ë°œìƒ",
                    gr.update(interactive=True),
                    gr.update(interactive=False),
                    f"ì˜¤ë¥˜: {str(e)}\n\n{traceback.format_exc()}",
                    gr.Dropdown.update()
                )

        # ì´ë²¤íŠ¸ ì—°ê²°
        search_btn.click(
            fn=search_models,
            inputs=[search_box, model_type_filter, language_filter, library_filter],
            outputs=model_list
        )
        
        model_list.select(
            fn=select_model,
            inputs=[model_list],
            outputs=selected_model
        )
        
        use_auth.change(
            fn=toggle_auth,
            inputs=use_auth,
            outputs=auth_column
        )
        
        download_btn.click(
            fn=download_model_with_progress,
            inputs=[
                selected_model,
                target_path,
                use_auth,
                hf_token
            ],
            outputs=[
                download_status,
                download_btn,
                cancel_btn,
                download_info,
                model_dropdown
            ]
        )
        
    with gr.Tab("ìºì‹œ"):
        with gr.Row():
            with gr.Column():
                refresh_button = gr.Button("ëª¨ë¸ ëª©ë¡ ìƒˆë¡œê³ ì¹¨")
                refresh_info = gr.Textbox(label="ìƒˆë¡œê³ ì¹¨ ê²°ê³¼", interactive=False)
            with gr.Column():
                clear_all_btn = gr.Button("ëª¨ë“  ëª¨ë¸ ìºì‹œ ì‚­ì œ")
                clear_all_result = gr.Textbox(label="ê²°ê³¼", interactive=False)

        def refresh_model_list():
            """
            ìˆ˜ë™ ìƒˆë¡œê³ ì¹¨ ì‹œ í˜¸ì¶œë˜ëŠ” í•¨ìˆ˜.
            - ìƒˆë¡œ scan_local_models()
            - DropDown ëª¨ë¸ ëª©ë¡ ì—…ë°ì´íŠ¸
            """
            # ìƒˆë¡œ ìŠ¤ìº”
            new_local_models = get_all_local_models()
            # ìƒˆ choices: API ëª¨ë¸ + ë¡œì»¬ ëª¨ë¸ + ì‚¬ìš©ìž ì§€ì • ëª¨ë¸ ê²½ë¡œ ë³€ê²½
            api_models = [
                "gpt-3.5-turbo",
                "gpt-4o-mini",
                "gpt-4o"
                # í•„ìš” ì‹œ ì¶”ê°€
            ]
            local_models = new_local_models["transformers"] + new_local_models["gguf"] + new_local_models["mlx"]
            new_choices = api_models + local_models + ["ì‚¬ìš©ìž ì§€ì • ëª¨ë¸ ê²½ë¡œ ë³€ê²½"]
            new_choices = list(dict.fromkeys(new_choices))
            new_choices = sorted(new_choices)  # ì •ë ¬ ì¶”ê°€
            # ë°˜í™˜ê°’:
            logger.info("ëª¨ë¸ ëª©ë¡ ìƒˆë¡œê³ ì¹¨")
            return gr.update(choices=new_choices), "ëª¨ë¸ ëª©ë¡ì„ ìƒˆë¡œê³ ì¹¨ í–ˆìŠµë‹ˆë‹¤."
            
        refresh_button.click(
            fn=refresh_model_list,
            inputs=[],
            outputs=[model_dropdown, refresh_info]
        )
        clear_all_btn.click(
            fn=clear_all_model_cache,
            inputs=[],
            outputs=clear_all_result
        )
    with gr.Tab("ìœ í‹¸ë¦¬í‹°"):
        gr.Markdown("### ëª¨ë¸ ë¹„íŠ¸ ë³€í™˜ê¸°")
        gr.Markdown("Transformersì™€ BitsAndBytesë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ 8ë¹„íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.")
        
        with gr.Row():
            model_id = gr.Textbox(label="HuggingFace ëª¨ë¸ ID", placeholder="ì˜ˆ: gpt2")
            output_dir = gr.Textbox(label="ì €ìž¥ ë””ë ‰í† ë¦¬", placeholder="./converted_models/gpt2_8bit")
        with gr.Row():
            quant_type = gr.Radio(choices=["float8", "int8", "int4"], label="ë³€í™˜ ìœ í˜•", value="int8")
        with gr.Row():
            push_to_hub = gr.Checkbox(label="Hugging Face Hubì— í‘¸ì‹œ", value=False)
        
        convert_button = gr.Button("ëª¨ë¸ ë³€í™˜ ì‹œìž‘")
        output = gr.Textbox(label="ê²°ê³¼")
        
        convert_button.click(fn=convert_and_save, inputs=[model_id, output_dir, push_to_hub, quant_type], outputs=output)
    with gr.Tab("ì‚¬ìš©ìž ì§€ì • ëª¨ë¸"):
        gr.Markdown("### ì‚¬ìš©ìž ì§€ì • ëª¨ë¸ ê²½ë¡œ ì„¤ì •")
        custom_path_text = gr.Textbox(
            label="ì‚¬ìš©ìž ì§€ì • ëª¨ë¸ ê²½ë¡œ",
            placeholder="./models/custom-model",
        )
        apply_custom_path_btn = gr.Button("ê²½ë¡œ ì ìš©")

        # custom_path_text -> custom_model_path_state ì €ìž¥
        def update_custom_path(path):
            return path

        apply_custom_path_btn.click(
            fn=update_custom_path,
            inputs=[custom_path_text],
            outputs=[custom_model_path_state]
        )

demo.launch(debug=True, inbrowser=True, server_port=7861, width=500)