# app.py

import os
import shutil
import traceback
import torch
import gc
import gradio as gr
from transformers import AutoTokenizer, AutoModelForCausalLM
import openai
import logging
from logging.handlers import RotatingFileHandler
from model_handlers import (
    GGUFModelHandler,MiniCPMLlama3V25Handler, GLM4Handler, GLM4VHandler, VisionModelHandler,
    Aya23Handler, GLM4HfHandler, OtherModelHandler, QwenHandler, MlxModelHandler, MlxVisionHandler
)
import json
import datetime
import csv
import secrets
from huggingface_hub import HfApi, list_models
from utils import (
    make_local_dir_name,
    get_all_local_models,  # ìˆ˜ì •ëœ í•¨ìˆ˜
    scan_local_models,
    get_model_list_from_hf_hub,
    download_model_from_hf,
    ensure_model_available,
    convert_and_save,
    
)
from cache import models_cache
import sqlite3

def get_existing_sessions():
    """
    DBì—ì„œ ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ëª¨ë“  session_id ëª©ë¡ì„ ê°€ì ¸ì˜´ (ì¤‘ë³µ ì—†ì´).
    """
    try:
        conn = sqlite3.connect("chat_history.db")
        cursor = conn.cursor()
        cursor.execute("SELECT DISTINCT session_id FROM chat_history ORDER BY session_id ASC")
        rows = cursor.fetchall()
        conn.close()
        session_ids = [r[0] for r in rows]
        return session_ids
    except Exception as e:
        logger.error(f"ì„¸ì…˜ ëª©ë¡ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        return []
def save_chat_history_db(history, session_id="session_1"):
    """
    ì±„íŒ… íˆìŠ¤í† ë¦¬ë¥¼ SQLite DBì— ì €ì¥í•©ë‹ˆë‹¤.
    """
    try:
        conn = sqlite3.connect("chat_history.db")
        cursor = conn.cursor()
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS chat_history (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                session_id TEXT NOT NULL,
                role TEXT NOT NULL,
                content TEXT NOT NULL,
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        for msg in history:
            cursor.execute("""
                INSERT INTO chat_history (session_id, role, content)
                VALUES (?, ?, ?)
            """, (session_id, msg.get("role"), msg.get("content")))
        
        conn.commit()
        conn.close()
        logger.info(f"DBì— ì±„íŒ… íˆìŠ¤í† ë¦¬ ì €ì¥ ì™„ë£Œ (session_id={session_id})")
        return True
    except Exception as e:
        logger.error(f"DB ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
        return False
    
def save_chat_history(history):
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    file_name = f"chat_history_{timestamp}.json"
    try:
        with open(file_name, "w", encoding="utf-8") as f:
            json.dump(history, f, ensure_ascii=False, indent=2)
        logger.info(f"ì±„íŒ… íˆìŠ¤í† ë¦¬ ì €ì¥ ì™„ë£Œ: {file_name}")
        return file_name
    except Exception as e:
        logger.error(f"ì±„íŒ… íˆìŠ¤í† ë¦¬ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
        return None

def save_chat_history_csv(history):
    """
    ì±„íŒ… íˆìŠ¤í† ë¦¬ë¥¼ CSV í˜•íƒœë¡œ ì €ì¥
    """
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    file_name = f"chat_history_{timestamp}.csv"
    try:
        # CSV íŒŒì¼ ì—´ê¸°
        with open(file_name, "w", newline="", encoding="utf-8") as csvfile:
            writer = csv.writer(csvfile)
            # í—¤ë” ì‘ì„±
            writer.writerow(["role", "content"])
            # ê° ë©”ì‹œì§€ row ì‘ì„±
            for msg in history:
                writer.writerow([msg.get("role"), msg.get("content")])
        logger.info(f"ì±„íŒ… íˆìŠ¤í† ë¦¬ CSV ì €ì¥ ì™„ë£Œ: {file_name}")
        return file_name
    except Exception as e:
        logger.error(f"ì±„íŒ… íˆìŠ¤í† ë¦¬ CSV ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
        return None
    
def save_chat_button_click(history):
    if not history:
        return "ì±„íŒ… ì´ë ¥ì´ ì—†ìŠµë‹ˆë‹¤."
    saved_path = save_chat_history(history)
    if saved_path is None:
        return "âŒ ì±„íŒ… ê¸°ë¡ ì €ì¥ ì‹¤íŒ¨"
    else:
        return f"âœ… ì±„íŒ… ê¸°ë¡ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {saved_path}"
    
# ì˜ˆ: session_idë¥¼ í•¨ìˆ˜ ì¸ìë¡œ ì „ë‹¬ë°›ì•„ DBë¡œë¶€í„° í•´ë‹¹ ì„¸ì…˜ ë°ì´í„°ë§Œ ë¶ˆëŸ¬ì˜¤ê¸°
def load_chat_from_db(session_id):
    conn = sqlite3.connect("chat_history.db")
    cursor = conn.cursor()
    cursor.execute("SELECT role, content FROM chat_history WHERE session_id=? ORDER BY id ASC", (session_id,))
    rows = cursor.fetchall()
    conn.close()
    history = []
    for row in rows:
        role, content = row
        history.append({"role": role, "content": content})
    return history

##########################################
# 1) ìœ í‹¸ í•¨ìˆ˜ë“¤
##########################################

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ë¡œê·¸ í¬ë§· ì •ì˜
formatter = logging.Formatter(
    fmt='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

# ì½˜ì†” í•¸ë“¤ëŸ¬ ì¶”ê°€
console_handler = logging.StreamHandler()
console_handler.setFormatter(formatter)
logger.addHandler(console_handler)

# íŒŒì¼ í•¸ë“¤ëŸ¬ ì¶”ê°€ (ë¡œí…Œì´íŒ… íŒŒì¼ í•¸ë“¤ëŸ¬ ì‚¬ìš©)
log_file = "app.log"  # ì›í•˜ëŠ” ë¡œê·¸ íŒŒì¼ ê²½ë¡œë¡œ ë³€ê²½ ê°€ëŠ¥
rotating_file_handler = RotatingFileHandler(
    log_file, maxBytes=5*1024*1024, backupCount=5  # 5MBë§ˆë‹¤ ìƒˆë¡œìš´ íŒŒì¼ë¡œ êµì²´, ìµœëŒ€ 5ê°œ ë°±ì—…
)
rotating_file_handler.setFormatter(formatter)
logger.addHandler(rotating_file_handler)

# ë©”ëª¨ë¦¬ ìƒì— ë¡œë“œëœ ëª¨ë¸ë“¤ì„ ì €ì¥í•˜ëŠ” ìºì‹œ
LOCAL_MODELS_ROOT = "./models"

def build_model_cache_key(model_id: str, model_type: str, local_path: str = None) -> str:
    """
    models_cacheì— ì‚¬ìš©ë  keyë¥¼ êµ¬ì„±.
    - ë§Œì•½ model_id == 'Local (Custom Path)' ì´ê³  local_pathê°€ ì£¼ì–´ì§€ë©´ 'local::{local_path}'
    - ê·¸ ì™¸ì—ëŠ” 'auto::{model_type}::{local_dir}::hf::{model_id}' í˜•íƒœ.
    """
    if model_id == "Local (Custom Path)" and local_path:
        return f"local::{local_path}"
    elif model_type == "api":
        return f"api::{model_id}"
    else:
        local_dirname = make_local_dir_name(model_id)
        local_dirpath = os.path.join("./models", model_type, local_dirname)
        return f"auto::{model_type}::{local_dirpath}::hf::{model_id}"

def clear_model_cache(model_id: str, local_path: str = None) -> str:
    """
    íŠ¹ì • ëª¨ë¸ì— ëŒ€í•œ ìºì‹œë¥¼ ì œê±° (models_cacheì—ì„œ í•´ë‹¹ keyë¥¼ ì‚­ì œ).
    - ë§Œì•½ í•´ë‹¹ keyê°€ ì—†ìœ¼ë©´ 'ì´ë¯¸ ì—†ìŒ' ë©”ì‹œì§€ ë°˜í™˜
    - ì„±ê³µ ì‹œ 'ìºì‹œ ì‚­ì œ ì™„ë£Œ' ë©”ì‹œì§€
    """
    # ëª¨ë¸ ìœ í˜•ì„ ê²°ì •í•´ì•¼ í•©ë‹ˆë‹¤.
    if model_id in ["gpt-3.5-turbo", "gpt-4o", "gpt-4o-mini"]:
        model_type = "api"
    else:
        # ë¡œì»¬ ëª¨ë¸ì˜ ê¸°ë³¸ ìœ í˜•ì„ transformersë¡œ ì„¤ì • (í•„ìš” ì‹œ ìˆ˜ì •)
        model_type = "transformers"
    key = build_model_cache_key(model_id, model_type, local_path)
    if key in models_cache:
        del models_cache[key]
        msg = f"[cache] ëª¨ë¸ ìºì‹œ ì œê±°: {key}"
        logger.info(msg)
        return msg
    else:
        msg = f"[cache] ì´ë¯¸ ìºì‹œì— ì—†ê±°ë‚˜, ë¡œë“œëœ ì  ì—†ìŒ: {key}"
        logger.info(msg)
        return msg

def refresh_model_list():
    new_local_models = get_all_local_models()
    api_models = ["gpt-3.5-turbo", "gpt-4o-mini", "gpt-4o"]
    local_models = (
        new_local_models["transformers"] + 
        new_local_models["gguf"] + 
        new_local_models["mlx"]
    )
    new_choices = api_models + local_models
    new_choices = sorted(list(dict.fromkeys(new_choices)))
    return gr.update(choices=new_choices), "ëª¨ë¸ ëª©ë¡ì„ ìƒˆë¡œê³ ì¹¨í–ˆìŠµë‹ˆë‹¤."


def clear_all_model_cache():
    """
    í˜„ì¬ ë©”ëª¨ë¦¬ì— ë¡œë“œëœ ëª¨ë“  ëª¨ë¸ ìºì‹œ(models_cache)ë¥¼ í•œ ë²ˆì— ì‚­ì œ.
    í•„ìš”í•˜ë‹¤ë©´, ë¡œì»¬ í´ë”ì˜ .cacheë“¤ë„ ì¼ê´„ ì‚­ì œí•  ìˆ˜ ìˆìŒ.
    """
    for key, handler in list(models_cache.items()):
        # í˜¹ì‹œ model, tokenizer ë“± ë©”ëª¨ë¦¬ë¥¼ ì ìœ í•˜ëŠ” ì†ì„±ì´ ìˆìœ¼ë©´ ì œê±°
        if hasattr(handler, "model"):
            del handler.model
        if hasattr(handler, "tokenizer"):
            del handler.tokenizer
        # í•„ìš” ì‹œ handler ë‚´ë¶€ì˜ ë‹¤ë¥¸ ìì›ë“¤(ì˜ˆ: embeddings ë“±)ë„ ì •ë¦¬
        
    # 1) ë©”ëª¨ë¦¬ ìºì‹œ ì „ë¶€ ì‚­ì œ
    count = len(models_cache)
    models_cache.clear()
    logger.info(f"[*] ë©”ëª¨ë¦¬ ìºì‹œ ì‚­ì œ: {count}ê°œ ëª¨ë¸")

    # 2) (ì„ íƒ) ë¡œì»¬ í´ë” .cache ì‚­ì œ
    #    ì˜ˆ: ./models/*/.cache í´ë” ì „ë¶€ ì‚­ì œ
    #    ì›ì¹˜ ì•Šìœ¼ë©´ ì£¼ì„ì²˜ë¦¬
    
    
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    elif torch.backends.mps.is_available():
        torch.mps.empty_cache()
        
    gc.collect()
        
    cache_deleted = 0
    for subdir, models in get_all_local_models().items():
        for folder in models:
            folder_path = os.path.join(LOCAL_MODELS_ROOT, subdir, folder)
            if os.path.isdir(folder_path):
                cache_path = os.path.join(folder_path, ".cache")
                if os.path.isdir(cache_path):
                    shutil.rmtree(cache_path)
                    cache_deleted += 1
    logger.info(f"[*] ë¡œì»¬ í´ë” .cache ì‚­ì œ: {cache_deleted}ê°œ í´ë” ì‚­ì œ")
    return f"[cache all] {count}ê°œ ëª¨ë¸ ìºì‹œ ì‚­ì œ ì™„ë£Œ. ë¡œì»¬ í´ë” .cache {cache_deleted}ê°œ ì‚­ì œ."

##########################################
# 2) ëª¨ë¸ ë¡œë“œ & ì¶”ë¡  ë¡œì§
##########################################

def load_model(selected_model, model_type, quantization_bit="Q8_0", local_model_path=None, api_key=None):
    """
    ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜. íŠ¹ì • ëª¨ë¸ì— ëŒ€í•œ ë¡œë“œ ë¡œì§ì„ ì™¸ë¶€ í•¸ë“¤ëŸ¬ë¡œ ë¶„ë¦¬.
    """
    model_id = selected_model
    if model_type not in ["transformers", "gguf", "mlx", "api"]:
        logger.error(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ëª¨ë¸ ìœ í˜•: {model_type}")
        return None
    if model_type == "api":
        # API ëª¨ë¸ì€ ë³„ë„ì˜ ë¡œë“œê°€ í•„ìš” ì—†ìœ¼ë¯€ë¡œ í•¸ë“¤ëŸ¬ ìƒì„± ì•ˆí•¨
        return None
    if model_type == "gguf":
        # GGUF ëª¨ë¸ ë¡œë”© ë¡œì§
        if not ensure_model_available(model_id, local_model_path, model_type):
            logger.error(f"ëª¨ë¸ '{model_id}'ì„(ë¥¼) ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
        handler = GGUFModelHandler(
            model_id=model_id,
            quantization_bit=quantization_bit,
            local_model_path=local_model_path,
            model_type=model_type
        )
        models_cache[build_model_cache_key(model_id, model_type, quantization_bit, local_model_path)] = handler
        return handler
    elif model_type == "mlx":
        if "vision" in model_id.lower() or "qwen2-vl" in model_id.lower():
            if not ensure_model_available(model_id, local_model_path, model_type):
                logger.error(f"ëª¨ë¸ '{model_id}'ì„(ë¥¼) ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
            handler = MlxVisionHandler(
                model_id=model_id,
                local_model_path=local_model_path,
                model_type=model_type
            )
            models_cache[build_model_cache_key(model_id, model_type)] = handler
            return handler
        else:
            if not ensure_model_available(model_id, local_model_path, model_type):
                logger.error(f"ëª¨ë¸ '{model_id}'ì„(ë¥¼) ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
            handler = MlxModelHandler(
                model_id=model_id,
                local_model_path=local_model_path,
                model_type=model_type
            )
            models_cache[build_model_cache_key(model_id, model_type)] = handler
            return handler
    else:
        if model_id == "openbmb/MiniCPM-Llama3-V-2_5":
            # ëª¨ë¸ ì¡´ì¬ í™•ì¸ ë° ë‹¤ìš´ë¡œë“œ
            if not ensure_model_available(model_id, local_model_path, model_type):
                logger.error(f"ëª¨ë¸ '{model_id}'ì„(ë¥¼) ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
            handler = MiniCPMLlama3V25Handler(
                model_id=model_id,
                local_model_path=local_model_path,
                model_type=model_type
            )
            models_cache[build_model_cache_key(model_id, model_type)] = handler
            return handler
        elif model_id in [
            "Bllossom/llama-3.2-Korean-Bllossom-AICA-5B",
        ] or ("vision" in model_id.lower() and model_id != "Bllossom/llama-3.1-Korean-Bllossom-Vision-8B"):
            # ëª¨ë¸ ì¡´ì¬ í™•ì¸ ë° ë‹¤ìš´ë¡œë“œ
            if not ensure_model_available(model_id, local_model_path, model_type):
                logger.error(f"ëª¨ë¸ '{model_id}'ì„(ë¥¼) ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
            handler = VisionModelHandler(
                model_id=model_id,
                local_model_path=local_model_path,
                model_type=model_type
            )
            models_cache[build_model_cache_key(model_id, model_type)] = handler
            return handler
        elif model_id == "THUDM/glm-4v-9b":
            # ëª¨ë¸ ì¡´ì¬ í™•ì¸ ë° ë‹¤ìš´ë¡œë“œ
            if not ensure_model_available(model_id, local_model_path, model_type):
                logger.error(f"ëª¨ë¸ '{model_id}'ì„(ë¥¼) ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
            handler = GLM4VHandler(
                model_id=model_id,
                local_model_path=local_model_path,
                model_type=model_type
            )
            models_cache[build_model_cache_key(model_id, model_type)] = handler
            return handler
        elif model_id == "THUDM/glm-4-9b-chat":
            if not ensure_model_available(model_id, local_model_path, model_type):
                logger.error(f"ëª¨ë¸ '{model_id}'ì„(ë¥¼) ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
            handler = GLM4Handler(
                model_id=model_id,
                local_model_path=local_model_path,
                model_type=model_type
            )
            models_cache[build_model_cache_key(model_id, model_type)] = handler
            return handler
        elif model_id in ["THUDM/glm-4-9b-chat-hf", "THUDM/glm-4-9b-chat-1m-hf"]:
            if not ensure_model_available(model_id, local_model_path, model_type):
                logger.error(f"ëª¨ë¸ '{model_id}'ì„(ë¥¼) ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
            handler = GLM4HfHandler(
                model_id=model_id,  # model_idê°€ ì •ì˜ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
                local_model_path=local_model_path,
                model_type=model_type
            )
            models_cache[build_model_cache_key(model_id, model_type)] = handler
            return handler
        elif model_id in ["bean980310/glm-4-9b-chat-hf_float8", "genai-archive/glm-4-9b-chat-hf_int8"]:
            # 'fp8' íŠ¹í™” í•¸ë“¤ëŸ¬ ë¡œì§ ì¶”ê°€
            if not ensure_model_available(model_id, local_model_path, model_type):
                logger.error(f"ëª¨ë¸ '{model_id}'ì„(ë¥¼) ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
            handler = GLM4HfHandler(
                model_id=model_id,  # model_idê°€ ì •ì˜ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
                local_model_path=local_model_path,
                model_type=model_type
            )
            models_cache[build_model_cache_key(model_id, model_type)] = handler
            return handler
        elif model_id in ["CohereForAI/aya-23-8B", "CohereForAI/aya-23-35B"]:
            if not ensure_model_available(model_id, local_model_path, model_type):
                logger.error(f"ëª¨ë¸ '{model_id}'ì„(ë¥¼) ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
            handler = Aya23Handler(
                model_id=model_id,  # model_idê°€ ì •ì˜ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
                local_model_path=local_model_path,
                model_type=model_type
            )
            models_cache[build_model_cache_key(model_id, model_type)] = handler
            return handler
        elif "qwen" in model_id.lower():
            if not ensure_model_available(model_id, local_model_path, model_type):
                logger.error(f"ëª¨ë¸ '{model_id}'ì„(ë¥¼) ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
            handler = QwenHandler(
                model_id=model_id,  # model_idê°€ ì •ì˜ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
                local_model_path=local_model_path,
                model_type=model_type
            )
            models_cache[build_model_cache_key(model_id, model_type)] = handler
            return handler
        else:
            if not ensure_model_available(model_id, local_model_path, model_type):
                logger.error(f"ëª¨ë¸ '{model_id}'ì„(ë¥¼) ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
            handler = OtherModelHandler(model_id, local_model_path=local_model_path, model_type=model_type)
            models_cache[build_model_cache_key(model_id, model_type)] = handler
            return handler

def generate_answer(history, selected_model, model_type, local_model_path=None, image_input=None, api_key=None):
    """
    ì‚¬ìš©ì íˆìŠ¤í† ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ ìƒì„±.
    """
    if not history:
        system_message = {
            "role": "system",
            "content": "ë‹¹ì‹ ì€ ìœ ìš©í•œ AI ë¹„ì„œì…ë‹ˆë‹¤."
        }
        history = [system_message]
    
    cache_key = build_model_cache_key(selected_model, model_type, local_path=local_model_path)
    handler = models_cache.get(cache_key)
    
    if model_type == "api":
        if not api_key:
            logger.error("OpenAI API Keyê°€ missing.")
            return "OpenAI API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤."
        openai.api_key = api_key
        messages = [{"role": msg['role'], "content": msg['content']} for msg in history]
        logger.info(f"[*] OpenAI API ìš”ì²­: {messages}")
        
        try:
            response = openai.ChatCompletion.create(
                model=selected_model,
                messages=messages,
                temperature=0.7,
                max_tokens=1024,
                top_p=0.9
            )
            answer = response.choices[0].message["content"]
            logger.info(f"[*] OpenAI ì‘ë‹µ: {answer}")
            return answer
        except Exception as e:
            logger.error(f"OpenAI API ì˜¤ë¥˜: {str(e)}\n\n{traceback.format_exc()}")
            return f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}\n\n{traceback.format_exc()}"
    
    else:
        if not handler:
            logger.info(f"[*] ëª¨ë¸ ë¡œë“œ ì¤‘: {selected_model}")
            handler = load_model(selected_model, model_type, local_model_path=local_model_path)
        
        if not handler:
            logger.error("ëª¨ë¸ í•¸ë“¤ëŸ¬ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return "ëª¨ë¸ í•¸ë“¤ëŸ¬ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        logger.info(f"[*] Generating answer using {handler.__class__.__name__}")
        try:
            if isinstance(handler, VisionModelHandler):
                answer = handler.generate_answer(history, image_input)
            else:
                answer = handler.generate_answer(history)
            return answer
        except Exception as e:
            logger.error(f"ëª¨ë¸ ì¶”ë¡  ì˜¤ë¥˜: {str(e)}\n\n{traceback.format_exc()}")
            return f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}\n\n{traceback.format_exc()}"

##########################################
# 3) Gradio UI
##########################################

with gr.Blocks() as demo:
    gr.Markdown("## ê°„ë‹¨í•œ Chatbot")
    api_models = [
        "gpt-3.5-turbo",
        "gpt-4o-mini",
        "gpt-4o"
        # í•„ìš” ì‹œ ì¶”ê°€
    ]
    
    # HuggingFaceì—ì„œ ì§€ì›í•˜ëŠ” ê¸°ë³¸ ëª¨ë¸ ëª©ë¡
    known_hf_models = [
        "meta-llama/Llama-3.1-8B",
        "meta-llama/Llama-3.2-11B-Vision",
        "meta-llama/Llama-3.2-11B-Vision-Instruct",
        "openbmb/MiniCPM-Llama3-V-2_5",
        "Bllossom/llama-3.2-Korean-Bllossom-3B",
        "Bllossom/llama-3.2-Korean-Bllossom-AICA-5B",
        "Bllossom/llama-3.1-Korean-Bllossom-Vision-8B",
        "THUDM/glm-4-9b-chat",
        "THUDM/glm-4-9b-chat-hf",
        "THUDM/glm-4-9b-chat-1m",
        "THUDM/glm-4-9b-chat-1m-hf",
        "THUDM/glm-4v-9b",
        "huggyllama/llama-7b",
        "OrionStarAI/Orion-14B-Base",
        "OrionStarAI/Orion-14B-Chat",
        "OrionStarAI/Orion-14B-LongChat",
        "CohereForAI/aya-23-8B",
        "CohereForAI/aya-23-35B",
        "Qwen/Qwen2.5-7B",
        "Qwen/Qwen2.5-7B-Instruct",
        "Qwen/Qwen2.5-14B",
        "Qwen/Qwen2.5-14B-Instruct",
        "EleutherAI/polyglot-ko-1.3b"
    ]
    
    local_models_data = get_all_local_models()
    transformers_local = local_models_data["transformers"]
    gguf_local = local_models_data["gguf"]
    mlx_local = local_models_data["mlx"]
    
    custom_model_path_state = gr.State("")
    session_id_state = gr.State(None)
    system_message_box = gr.Textbox(
        label="ì‹œìŠ¤í…œ ë©”ì‹œì§€",
        value="ë‹¹ì‹ ì€ ìœ ìš©í•œ AI ë¹„ì„œì…ë‹ˆë‹¤.",
        placeholder="ëŒ€í™”ì˜ ì„±ê²©, ë§íˆ¬ ë“±ì„ ì •ì˜í•˜ì„¸ìš”."
    )
        
    with gr.Tab("ë©”ì¸"):
        
        history_state = gr.State([])
        
        initial_choices = api_models + transformers_local + gguf_local + mlx_local + ["ì‚¬ìš©ì ì§€ì • ëª¨ë¸ ê²½ë¡œ ë³€ê²½"]
        initial_choices = list(dict.fromkeys(initial_choices))
        initial_choices = sorted(initial_choices)  # ì •ë ¬ ì¶”ê°€
        
        with gr.Row():
            model_type_dropdown = gr.Radio(
                label="ëª¨ë¸ ìœ í˜• ì„ íƒ",
                choices=["all", "transformers", "gguf", "mlx"],
                value="all",
            )
        
        model_dropdown = gr.Dropdown(
            label="ëª¨ë¸ ì„ íƒ",
            choices=initial_choices,
            value=initial_choices[0] if len(initial_choices) > 0 else None,
        )
        
        api_key_text = gr.Textbox(
            label="OpenAI API Key",
            placeholder="sk-...",
            visible=False  # ê¸°ë³¸ì ìœ¼ë¡œ ìˆ¨ê¹€
        )
        image_info = gr.Markdown("", visible=False)
        with gr.Column():
            with gr.Row():
                image_input = gr.Image(label="ì´ë¯¸ì§€ ì—…ë¡œë“œ (ì„ íƒ)", type="pil", visible=False)
                chatbot = gr.Chatbot(height=400, label="Chatbot", type="messages")
            with gr.Row():
                msg = gr.Textbox(
                    label="ë©”ì‹œì§€ ì…ë ¥",
                    placeholder="ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”...",
                    scale=9
                )
                send_btn = gr.Button(
                    "ì „ì†¡",
                    scale=1,
                    variant="primary"
                )
            with gr.Row():
                status_text = gr.Markdown("", elem_id="status_text")
        history_state = gr.State([])
        
        # í•¨ìˆ˜: OpenAI API Keyì™€ ì‚¬ìš©ì ì§€ì • ëª¨ë¸ ê²½ë¡œ í•„ë“œì˜ ê°€ì‹œì„± ì œì–´
        def toggle_api_key_visibility(selected_model):
            """
            OpenAI API Key ì…ë ¥ í•„ë“œì˜ ê°€ì‹œì„±ì„ ì œì–´í•©ë‹ˆë‹¤.
            """
            api_visible = selected_model in api_models
            return gr.update(visible=api_visible)

        def toggle_image_input_visibility(selected_model):
            """
            ì´ë¯¸ì§€ ì…ë ¥ í•„ë“œì˜ ê°€ì‹œì„±ì„ ì œì–´í•©ë‹ˆë‹¤.
            """
            image_visible = (
                "vision" in selected_model.lower() or
                "qwen2-vl" in selected_model.lower() or
                selected_model in [
                    "Bllossom/llama-3.2-Korean-Bllossom-AICA-5B",
                    "THUDM/glm-4v-9b",
                    "openbmb/MiniCPM-Llama3-V-2_5"
                ]
            )
            return gr.update(visible=image_visible)
        
        # ëª¨ë¸ ì„ íƒ ë³€ê²½ ì‹œ ê°€ì‹œì„± í† ê¸€
        model_dropdown.change(
            fn=lambda selected_model: (
                toggle_api_key_visibility(selected_model),
                toggle_image_input_visibility(selected_model)
            ),
            inputs=[model_dropdown],
            outputs=[api_key_text, image_input]
        )
        def update_model_list(selected_type):
            local_models_data = get_all_local_models()
            transformers_local = local_models_data["transformers"]
            gguf_local = local_models_data["gguf"]
            mlx_local = local_models_data["mlx"]
            
            # "ì „ì²´ ëª©ë¡"ì´ë©´ => API ëª¨ë¸ + ëª¨ë“  ë¡œì»¬ ëª¨ë¸ + "ì‚¬ìš©ì ì§€ì • ëª¨ë¸ ê²½ë¡œ ë³€ê²½"
            if selected_type == "all":
                all_models = api_models + transformers_local + gguf_local + mlx_local + ["ì‚¬ìš©ì ì§€ì • ëª¨ë¸ ê²½ë¡œ ë³€ê²½"]
                # ì¤‘ë³µ ì œê±° í›„ ì •ë ¬
                all_models = sorted(list(dict.fromkeys(all_models)))
                return gr.update(choices=all_models, value=all_models[0] if all_models else None)
            
            # ê°œë³„ í•­ëª©ì´ë©´ => í•´ë‹¹ ìœ í˜•ì˜ ë¡œì»¬ ëª¨ë¸ + "ì‚¬ìš©ì ì§€ì • ëª¨ë¸ ê²½ë¡œ ë³€ê²½"ë§Œ
            if selected_type == "transformers":
                updated_list = transformers_local + ["ì‚¬ìš©ì ì§€ì • ëª¨ë¸ ê²½ë¡œ ë³€ê²½"]
            elif selected_type == "gguf":
                updated_list = gguf_local + ["ì‚¬ìš©ì ì§€ì • ëª¨ë¸ ê²½ë¡œ ë³€ê²½"]
            elif selected_type == "mlx":
                updated_list = mlx_local + ["ì‚¬ìš©ì ì§€ì • ëª¨ë¸ ê²½ë¡œ ë³€ê²½"]
            else:
                # í˜¹ì‹œ ì˜ˆìƒì¹˜ ëª»í•œ ê°’ì´ë©´ transformersë¡œ ì²˜ë¦¬(ë˜ëŠ” None)
                updated_list = transformers_local + ["ì‚¬ìš©ì ì§€ì • ëª¨ë¸ ê²½ë¡œ ë³€ê²½"]
            
            updated_list = sorted(list(dict.fromkeys(updated_list)))
            return gr.update(choices=updated_list, value=updated_list[0] if updated_list else None)
        
        model_type_dropdown.change(
            fn=update_model_list,
            inputs=[model_type_dropdown],
            outputs=[model_dropdown]
        )
        
        def on_app_start():
            """
            Gradio ì•±ì´ ë¡œë“œë˜ë©´ì„œ ì‹¤í–‰ë  ì½œë°±.
            - ì„¸ì…˜ IDë¥¼ ì •í•˜ê³ ,
            - í•´ë‹¹ ì„¸ì…˜ì˜ íˆìŠ¤í† ë¦¬ë¥¼ DBì—ì„œ ë¶ˆëŸ¬ì˜¨ ë’¤ ë°˜í™˜.
            """
            # ì—¬ê¸°ì„œ í•„ìš”í•œ í…Œì´ë¸” ìƒì„± ë“±ì„ í•´ë„ ë¨
            # create_table_if_not_exists()  # í•„ìš” ì‹œ êµ¬í˜„
            
            sid = "demo_session"  # ë°ëª¨ìš©ìœ¼ë¡œ ê³ ì •. ì‹¤ì œë¡œëŠ” secrets.token_hex() ë“±ì„ ì“¸ ìˆ˜ ìˆìŒ.
            loaded_history = load_chat_from_db(sid)
            # ë§Œì•½ DBì— ê¸°ë¡ì´ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ê°€ ë°˜í™˜ë  ê²ƒ
            return sid, loaded_history
        
        # .load()ë¥¼ ì‚¬ìš©í•´, í˜ì´ì§€ ë¡œë”©ì‹œ ìë™ìœ¼ë¡œ on_app_start()ê°€ ì‹¤í–‰ë˜ë„ë¡ ì—°ê²°
        demo.load(
            fn=on_app_start,
            inputs=[],
            outputs=[session_id_state, history_state],
            queue=False
        )
        
        def user_message(user_input, history, system_msg):
            if not user_input.strip():
                return "", history, ""
            if not history:
                system_message = {
                    "role": "system",
                    "content": system_msg
                }
                history = [system_message]
            history.append({"role": "user", "content": user_input})
            return "", history, "ğŸ¤” ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."
    
        def bot_message(session_id, history, selected_model, custom_path, image, api_key):
            # ëª¨ë¸ ìœ í˜• ê²°ì •
            local_model_path = None
            if selected_model in api_models:
                model_type = "api"
                local_model_path = None
            elif selected_model == "ì‚¬ìš©ì ì§€ì • ëª¨ë¸ ê²½ë¡œ ë³€ê²½":
                # ì‚¬ìš©ì ì§€ì • ëª¨ë¸ ê²½ë¡œ ì‚¬ìš©
                model_type = "transformers"  # ê¸°ë³¸ ëª¨ë¸ ìœ í˜• ì„¤ì •, í•„ìš” ì‹œ ìˆ˜ì •
                local_model_path = custom_path
            else:
                # ë¡œì»¬ ëª¨ë¸ ìœ í˜• ê²°ì • (transformers, gguf, mlx)
                if selected_model in transformers_local:
                    model_type = "transformers"
                elif selected_model in gguf_local:
                    model_type = "gguf"
                elif selected_model in mlx_local:
                    model_type = "mlx"
                else:
                    model_type = "transformers"  # ê¸°ë³¸ê°’
                local_model_path = None  # ê¸°ë³¸ ë¡œì»¬ ê²½ë¡œ ì‚¬ìš©
                
            try:
                answer = generate_answer(history, selected_model, model_type, local_model_path, image, api_key)
            except Exception as e:
                answer = f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}\n\n{traceback.format_exc()}"
                
            history.append({"role": "assistant", "content": answer})
            
            save_chat_history_db(history, session_id=session_id)
            return history, ""  # ë¡œë”© ìƒíƒœ ì œê±°
    

        def filter_messages_for_chatbot(history):
            messages_for_chatbot = []
            for msg in history:
                if msg["role"] in ("user", "assistant"):
                    content = msg["content"] or ""
                    messages_for_chatbot.append({"role": msg["role"], "content": content})
            return messages_for_chatbot

        # ë©”ì‹œì§€ ì „ì†¡ ì‹œ í•¨ìˆ˜ ì—°ê²°
        msg.submit(
            fn=user_message,
            inputs=[msg, session_id_state, history_state, system_message_box],  # ì„¸ ë²ˆì§¸ íŒŒë¼ë¯¸í„° ì¶”ê°€
            outputs=[msg, history_state, status_text],
            queue=False
        ).then(
            fn=bot_message,
            inputs=[session_id_state, history_state, model_dropdown, custom_model_path_state, image_input, api_key_text],
            outputs=[history_state, status_text],
            queue=True
        ).then(
            fn=filter_messages_for_chatbot,
            inputs=[history_state],
            outputs=chatbot,
            queue=False
        )
        send_btn.click(
            fn=user_message,
            inputs=[msg, session_id_state, history_state, system_message_box],
            outputs=[msg, history_state, status_text],
            queue=False
        ).then(
            fn=bot_message,
            inputs=[session_id_state, history_state, model_dropdown, custom_model_path_state, image_input, api_key_text],
            outputs=[history_state, status_text],
            queue=True
        ).then(
            fn=filter_messages_for_chatbot,            # ì¶”ê°€ëœ ë¶€ë¶„
            inputs=[history_state],
            outputs=chatbot,                           # chatbotì— ìµœì¢… ì „ë‹¬
            queue=False
        )
    
    with gr.Tab("ë‹¤ìš´ë¡œë“œ"):
        gr.Markdown("""### ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
        HuggingFaceì—ì„œ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  ë¡œì»¬ì— ì €ì¥í•©ë‹ˆë‹¤. 
        ë¯¸ë¦¬ ì •ì˜ëœ ëª¨ë¸ ëª©ë¡ì—ì„œ ì„ íƒí•˜ê±°ë‚˜, ì»¤ìŠ¤í…€ ëª¨ë¸ IDë¥¼ ì§ì ‘ ì…ë ¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.""")
        
        with gr.Column():
            # ë‹¤ìš´ë¡œë“œ ëª¨ë“œ ì„ íƒ (ë¼ë””ì˜¤ ë²„íŠ¼)
            download_mode = gr.Radio(
                label="ë‹¤ìš´ë¡œë“œ ë°©ì‹ ì„ íƒ",
                choices=["Predefined", "Custom Repo ID"],
                value="Predefined",
                container=True,
            )
            # ëª¨ë¸ ì„ íƒ/ì…ë ¥ ì˜ì—­
            with gr.Column(visible=True) as predefined_column:
                predefined_dropdown = gr.Dropdown(
                    label="ëª¨ë¸ ì„ íƒ",
                    choices=sorted(known_hf_models),
                    value=known_hf_models[0] if known_hf_models else None,
                    info="ì§€ì›ë˜ëŠ” ëª¨ë¸ ëª©ë¡ì…ë‹ˆë‹¤."
                )
                
            with gr.Column(visible=False) as custom_column:
                custom_repo_id_box = gr.Textbox(
                    label="Custom Model ID",
                    placeholder="ì˜ˆ) facebook/opt-350m",
                    info="HuggingFaceì˜ ëª¨ë¸ IDë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: organization/model-name)"
                )
                
            # ë‹¤ìš´ë¡œë“œ ì„¤ì •
            with gr.Row():
                with gr.Column(scale=2):
                    target_path = gr.Textbox(
                        label="ì €ì¥ ê²½ë¡œ",
                        placeholder="./models/my-model",
                        value="",
                        interactive=True,
                        info="ë¹„ì›Œë‘ë©´ ìë™ìœ¼ë¡œ ê²½ë¡œê°€ ìƒì„±ë©ë‹ˆë‹¤."
                    )
                with gr.Column(scale=1):
                    use_auth = gr.Checkbox(
                        label="ì¸ì¦ í•„ìš”",
                        value=False,
                        info="ë¹„ê³µê°œ ë˜ëŠ” gated ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œ ì²´í¬"
                    )
            
            with gr.Column(visible=False) as auth_column:
                hf_token = gr.Textbox(
                    label="HuggingFace Token",
                    placeholder="hf_...",
                    type="password",
                    info="HuggingFaceì—ì„œ ë°œê¸‰ë°›ì€ í† í°ì„ ì…ë ¥í•˜ì„¸ìš”."
                )
            
            # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ê³¼ ì§„í–‰ ìƒíƒœ
            with gr.Row():
                download_btn = gr.Button(
                    "ë‹¤ìš´ë¡œë“œ ì‹œì‘",
                    variant="primary",
                    scale=2
                )
                cancel_btn = gr.Button(
                    "ì·¨ì†Œ",
                    variant="stop",
                    scale=1,
                    interactive=False
                )
                
            # ìƒíƒœ í‘œì‹œ
            download_status = gr.Markdown("")
            progress_bar = gr.Progress(
                track_tqdm=True,  # tqdm progress barsë¥¼ ì¶”ì 
            )
            
            # ë‹¤ìš´ë¡œë“œ ê²°ê³¼ì™€ ë¡œê·¸
            with gr.Accordion("ìƒì„¸ ì •ë³´", open=False):
                download_info = gr.TextArea(
                    label="ë‹¤ìš´ë¡œë“œ ë¡œê·¸",
                    interactive=False,
                    max_lines=10,
                    autoscroll=True
                )

        # UI ë™ì‘ ì œì–´ë¥¼ ìœ„í•œ í•¨ìˆ˜ë“¤
        def toggle_download_mode(mode):
            """ë‹¤ìš´ë¡œë“œ ëª¨ë“œì— ë”°ë¼ UI ì»´í¬ë„ŒíŠ¸ í‘œì‹œ/ìˆ¨ê¹€"""
            return [
                gr.update(visible=(mode == "Predefined")),  # predefined_column
                gr.update(visible=(mode == "Custom Repo ID"))  # custom_column
            ]

        def toggle_auth(use_auth_val):
            """ì¸ì¦ í•„ìš” ì—¬ë¶€ì— ë”°ë¼ í† í° ì…ë ¥ì°½ í‘œì‹œ/ìˆ¨ê¹€"""
            return gr.update(visible=use_auth_val)

        def download_with_progress(mode, predefined_choice, custom_repo, target_dir, use_auth_val, token):
            try:
                repo_id = predefined_choice if mode == "Predefined" else custom_repo.strip()
                if not repo_id:
                    yield (
                        "âŒ ëª¨ë¸ IDë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.",  # status
                        gr.update(interactive=True),  # download_btn
                        gr.update(interactive=False),  # cancel_btn
                        "ë‹¤ìš´ë¡œë“œê°€ ì‹œì‘ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",  # download_info
                        gr.Dropdown.update()
                    )
                    return
                
                # ëª¨ë¸ ìœ í˜• ê²°ì •
                if "gguf" in repo_id.lower():
                    model_type = "gguf"
                elif "mlx" in repo_id.lower():
                    model_type = "mlx"
                else:
                    model_type = "transformers"

                # ì§„í–‰ ìƒíƒœ ì´ˆê¸°í™”
                yield (
                    "ğŸ”„ ë‹¤ìš´ë¡œë“œ ì¤€ë¹„ ì¤‘...",
                    gr.update(interactive=False),
                    gr.update(interactive=True),
                    f"ëª¨ë¸: {repo_id}\nì¤€ë¹„ ì¤‘...",
                    gr.Dropdown.update()
                )

                # ì‹¤ì œ ë‹¤ìš´ë¡œë“œ ìˆ˜í–‰
                yield (
                    "ğŸ”„ ë‹¤ìš´ë¡œë“œ ì¤‘...",
                    gr.update(interactive=False),
                    gr.update(interactive=True),
                    "ë‹¤ìš´ë¡œë“œë¥¼ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤...",
                    gr.Dropdown.update()
                )
                result = download_model_from_hf(
                    repo_id,
                    target_dir or os.path.join("./models", model_type, make_local_dir_name(repo_id)),
                    model_type=model_type
                )

                # ë‹¤ìš´ë¡œë“œ ì™„ë£Œ í›„ UI ì—…ë°ì´íŠ¸
                yield (
                    "âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!" if "ì‹¤íŒ¨" not in result else "âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨",
                    gr.update(interactive=True),
                    gr.update(interactive=False),
                    result,
                    gr.Dropdown.update(choices=sorted(api_models + get_all_local_models()["transformers"] + get_all_local_models()["gguf"] + get_all_local_models()["mlx"] + ["ì‚¬ìš©ì ì§€ì • ëª¨ë¸ ê²½ë¡œ ë³€ê²½"]))
                )

            except Exception as e:
                yield (
                    "âŒ ì˜¤ë¥˜ ë°œìƒ",
                    gr.update(interactive=True),
                    gr.update(interactive=False),
                    f"ì˜¤ë¥˜: {str(e)}\n\n{traceback.format_exc()}",
                    gr.Dropdown.update()
                )

        # ì´ë²¤íŠ¸ ì—°ê²°
        download_mode.change(
            fn=toggle_download_mode,
            inputs=download_mode,
            outputs=[predefined_column, custom_column]
        )
        
        use_auth.change(
            fn=toggle_auth,
            inputs=use_auth,
            outputs=[auth_column]
        )
        
        download_btn.click(
            fn=download_with_progress,
            inputs=[
                download_mode,
                predefined_dropdown,
                custom_repo_id_box,
                target_path,
                use_auth,
                hf_token
            ],
            outputs=[
                download_status,
                download_btn,
                cancel_btn,
                download_info,
                model_dropdown  # model_dropdownì„ ì—…ë°ì´íŠ¸í•˜ë„ë¡ ë³€ê²½
            ]
        )
    with gr.Tab("í—ˆë¸Œ"):
        gr.Markdown("""### í—ˆê¹…í˜ì´ìŠ¤ í—ˆë¸Œ ëª¨ë¸ ê²€ìƒ‰
        í—ˆê¹…í˜ì´ìŠ¤ í—ˆë¸Œì—ì„œ ëª¨ë¸ì„ ê²€ìƒ‰í•˜ê³  ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
        í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•˜ê±°ë‚˜ í•„í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì›í•˜ëŠ” ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.""")
        
        with gr.Row():
            search_box = gr.Textbox(
                label="ê²€ìƒ‰ì–´",
                placeholder="ëª¨ë¸ ì´ë¦„, íƒœê·¸ ë˜ëŠ” í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”",
                scale=4
            )
            search_btn = gr.Button("ê²€ìƒ‰", scale=1)
            
        with gr.Row():
            with gr.Column(scale=1):
                model_type_filter = gr.Dropdown(
                    label="ëª¨ë¸ ìœ í˜•",
                    choices=["All", "Text Generation", "Vision", "Audio", "Other"],
                    value="All"
                )
                language_filter = gr.Dropdown(
                    label="ì–¸ì–´",
                    choices=["All", "Korean", "English", "Chinese", "Japanese", "Multilingual"],
                    value="All"
                )
                library_filter = gr.Dropdown(
                    label="ë¼ì´ë¸ŒëŸ¬ë¦¬",
                    choices=["All", "Transformers", "GGUF", "MLX"],
                    value="All"
                )
            with gr.Column(scale=3):
                model_list = gr.Dataframe(
                    headers=["Model ID", "Description", "Downloads", "Likes"],
                    label="ê²€ìƒ‰ ê²°ê³¼",
                    interactive=False
                )
        
        with gr.Row():
            selected_model = gr.Textbox(
                label="ì„ íƒëœ ëª¨ë¸",
                interactive=False
            )
            
        # ë‹¤ìš´ë¡œë“œ ì„¤ì •
        with gr.Row():
            with gr.Column(scale=2):
                target_path = gr.Textbox(
                    label="ì €ì¥ ê²½ë¡œ",
                    placeholder="./models/my-model",
                    value="",
                    interactive=True,
                    info="ë¹„ì›Œë‘ë©´ ìë™ìœ¼ë¡œ ê²½ë¡œê°€ ìƒì„±ë©ë‹ˆë‹¤."
                )
            with gr.Column(scale=1):
                use_auth = gr.Checkbox(
                    label="ì¸ì¦ í•„ìš”",
                    value=False,
                    info="ë¹„ê³µê°œ ë˜ëŠ” gated ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œ ì²´í¬"
                )
        
        with gr.Column(visible=False) as auth_column:
            hf_token = gr.Textbox(
                label="HuggingFace Token",
                placeholder="hf_...",
                type="password",
                info="HuggingFaceì—ì„œ ë°œê¸‰ë°›ì€ í† í°ì„ ì…ë ¥í•˜ì„¸ìš”."
            )
        
        # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ê³¼ ì§„í–‰ ìƒíƒœ
        with gr.Row():
            download_btn = gr.Button(
                "ë‹¤ìš´ë¡œë“œ",
                variant="primary",
                scale=2
            )
            cancel_btn = gr.Button(
                "ì·¨ì†Œ",
                variant="stop",
                scale=1,
                interactive=False
            )
            
        # ìƒíƒœ í‘œì‹œ
        download_status = gr.Markdown("")
        progress_bar = gr.Progress(track_tqdm=True)
        
        # ë‹¤ìš´ë¡œë“œ ê²°ê³¼ì™€ ë¡œê·¸
        with gr.Accordion("ìƒì„¸ ì •ë³´", open=False):
            download_info = gr.TextArea(
                label="ë‹¤ìš´ë¡œë“œ ë¡œê·¸",
                interactive=False,
                max_lines=10,
                autoscroll=True
            )

        def search_models(query, model_type, language, library):
            """í—ˆê¹…í˜ì´ìŠ¤ í—ˆë¸Œì—ì„œ ëª¨ë¸ ê²€ìƒ‰"""
            try:
                api = HfApi()
                # ê²€ìƒ‰ í•„í„° êµ¬ì„±
                filter_str = ""
                if model_type != "All":
                    filter_str += f"task_{model_type.lower().replace(' ', '_')}"
                if language != "All":
                    if filter_str:
                        filter_str += " AND "
                    filter_str += f"language_{language.lower()}"
                if library != "All":
                    filter_str += f"library_{library.lower()}"
                
                # ëª¨ë¸ ê²€ìƒ‰ ìˆ˜í–‰
                models = api.list_models(
                    filter=filter_str if filter_str else None,
                    limit=100,
                    sort="lastModified",
                    direction=-1
                )
                
                filtered_models = [model for model in models if query.lower() in model.id.lower()]
                
                # ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ êµ¬ì„±
                model_list = []
                for model in filtered_models:
                    description = model.cardData.get('description', '') if model.cardData else 'No description available.'
                    short_description = (description[:100] + "...") if len(description) > 100 else description
                    model_list.append([
                        model.id,
                        short_description,
                        model.downloads,
                        model.likes
                    ])
                return model_list
            except Exception as e:
                logger.error(f"ëª¨ë¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\n\n{traceback.format_exc()}")
                return [["ì˜¤ë¥˜ ë°œìƒ", str(e), "", ""]]

        def select_model(evt: gr.SelectData, data):
            """ë°ì´í„°í”„ë ˆì„ì—ì„œ ëª¨ë¸ ì„ íƒ"""
            selected_model_id = data.at[evt.index[0], "Model ID"]  # ì„ íƒëœ í–‰ì˜ 'Model ID' ì»¬ëŸ¼ ê°’
            return selected_model_id
        
        def toggle_auth(use_auth_val):
            """ì¸ì¦ í•„ìš” ì—¬ë¶€ì— ë”°ë¼ í† í° ì…ë ¥ì°½ í‘œì‹œ/ìˆ¨ê¹€"""
            return gr.update(visible=use_auth_val)
        
        def download_model_with_progress(model_id, target_dir, use_auth_val, token, progress=gr.Progress()):
            """ì§„í–‰ë¥  í‘œì‹œì™€ í•¨ê»˜ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ìˆ˜í–‰"""
            try:
                if not model_id:
                    yield (
                        "âŒ ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”.",
                        gr.update(interactive=True),
                        gr.update(interactive=False),
                        "ë‹¤ìš´ë¡œë“œê°€ ì‹œì‘ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
                        gr.Dropdown.update()
                    )
                    return
                
                # ëª¨ë¸ ìœ í˜• ê²°ì •
                model_type = "transformers"  # ê¸°ë³¸ê°’
                if "gguf" in model_id.lower():
                    model_type = "gguf"
                elif "mlx" in model_id.lower():
                    model_type = "mlx"

                # ì§„í–‰ ìƒíƒœ ì´ˆê¸°í™”
                progress(0, desc="ì¤€ë¹„ ì¤‘...")
                yield (
                    "ğŸ”„ ë‹¤ìš´ë¡œë“œ ì¤€ë¹„ ì¤‘...",
                    gr.update(interactive=False),
                    gr.update(interactive=True),
                    f"ëª¨ë¸: {model_id}\nì¤€ë¹„ ì¤‘...",
                    gr.Dropdown.update()
                )

                # ì‹¤ì œ ë‹¤ìš´ë¡œë“œ ìˆ˜í–‰
                progress(0.5, desc="ë‹¤ìš´ë¡œë“œ ì¤‘...")
                result = download_model_from_hf(
                    model_id,
                    target_dir or os.path.join("./models", model_type, make_local_dir_name(model_id)),
                    model_type=model_type,
                    token=token if use_auth_val else None
                )

                # ë‹¤ìš´ë¡œë“œ ì™„ë£Œ í›„ UI ì—…ë°ì´íŠ¸
                progress(1.0, desc="ì™„ë£Œ")
                local_models_data = get_all_local_models()
                local_models = (
                    local_models_data["transformers"] +
                    local_models_data["gguf"] +
                    local_models_data["mlx"]
                )
                new_choices = api_models + local_models + ["ì‚¬ìš©ì ì§€ì • ëª¨ë¸ ê²½ë¡œ ë³€ê²½"]
                new_choices = list(dict.fromkeys(new_choices))
                new_choices = sorted(new_choices)

                yield (
                    "âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!" if "ì‹¤íŒ¨" not in result else "âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨",
                    gr.update(interactive=True),
                    gr.update(interactive=False),
                    result,
                    gr.Dropdown.update(choices=new_choices)
                )

            except Exception as e:
                yield (
                    "âŒ ì˜¤ë¥˜ ë°œìƒ",
                    gr.update(interactive=True),
                    gr.update(interactive=False),
                    f"ì˜¤ë¥˜: {str(e)}\n\n{traceback.format_exc()}",
                    gr.Dropdown.update()
                )

        # ì´ë²¤íŠ¸ ì—°ê²°
        search_btn.click(
            fn=search_models,
            inputs=[search_box, model_type_filter, language_filter, library_filter],
            outputs=model_list
        )
        
        model_list.select(
            fn=select_model,
            inputs=[model_list],
            outputs=selected_model
        )
        
        use_auth.change(
            fn=toggle_auth,
            inputs=use_auth,
            outputs=auth_column
        )
        
        download_btn.click(
            fn=download_model_with_progress,
            inputs=[
                selected_model,
                target_path,
                use_auth,
                hf_token
            ],
            outputs=[
                download_status,
                download_btn,
                cancel_btn,
                download_info,
                model_dropdown
            ]
        )
        
    with gr.Tab("ìºì‹œ"):
        with gr.Row():
            with gr.Column():
                refresh_button = gr.Button("ëª¨ë¸ ëª©ë¡ ìƒˆë¡œê³ ì¹¨")
                refresh_info = gr.Textbox(label="ìƒˆë¡œê³ ì¹¨ ê²°ê³¼", interactive=False)
            with gr.Column():
                clear_all_btn = gr.Button("ëª¨ë“  ëª¨ë¸ ìºì‹œ ì‚­ì œ")
                clear_all_result = gr.Textbox(label="ê²°ê³¼", interactive=False)

        def refresh_model_list():
            """
            ìˆ˜ë™ ìƒˆë¡œê³ ì¹¨ ì‹œ í˜¸ì¶œë˜ëŠ” í•¨ìˆ˜.
            - ìƒˆë¡œ scan_local_models()
            - DropDown ëª¨ë¸ ëª©ë¡ ì—…ë°ì´íŠ¸
            """
            # ìƒˆë¡œ ìŠ¤ìº”
            new_local_models = get_all_local_models()
            # ìƒˆ choices: API ëª¨ë¸ + ë¡œì»¬ ëª¨ë¸ + ì‚¬ìš©ì ì§€ì • ëª¨ë¸ ê²½ë¡œ ë³€ê²½
            api_models = [
                "gpt-3.5-turbo",
                "gpt-4o-mini",
                "gpt-4o"
                # í•„ìš” ì‹œ ì¶”ê°€
            ]
            local_models = new_local_models["transformers"] + new_local_models["gguf"] + new_local_models["mlx"]
            new_choices = api_models + local_models + ["ì‚¬ìš©ì ì§€ì • ëª¨ë¸ ê²½ë¡œ ë³€ê²½"]
            new_choices = list(dict.fromkeys(new_choices))
            new_choices = sorted(new_choices)  # ì •ë ¬ ì¶”ê°€
            # ë°˜í™˜ê°’:
            logger.info("ëª¨ë¸ ëª©ë¡ ìƒˆë¡œê³ ì¹¨")
            return gr.update(choices=new_choices), "ëª¨ë¸ ëª©ë¡ì„ ìƒˆë¡œê³ ì¹¨ í–ˆìŠµë‹ˆë‹¤."
            
        refresh_button.click(
            fn=refresh_model_list,
            inputs=[],
            outputs=[model_dropdown, refresh_info]
        )
        clear_all_btn.click(
            fn=clear_all_model_cache,
            inputs=[],
            outputs=clear_all_result
        )
    with gr.Tab("ìœ í‹¸ë¦¬í‹°"):
        gr.Markdown("### ëª¨ë¸ ë¹„íŠ¸ ë³€í™˜ê¸°")
        gr.Markdown("Transformersì™€ BitsAndBytesë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ 8ë¹„íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.")
        
        with gr.Row():
            model_id = gr.Textbox(label="HuggingFace ëª¨ë¸ ID", placeholder="ì˜ˆ: gpt2")
            output_dir = gr.Textbox(label="ì €ì¥ ë””ë ‰í† ë¦¬", placeholder="./converted_models/gpt2_8bit")
        with gr.Row():
            quant_type = gr.Radio(choices=["float8", "int8", "int4"], label="ë³€í™˜ ìœ í˜•", value="int8")
        with gr.Row():
            push_to_hub = gr.Checkbox(label="Hugging Face Hubì— í‘¸ì‹œ", value=False)
        
        convert_button = gr.Button("ëª¨ë¸ ë³€í™˜ ì‹œì‘")
        output = gr.Textbox(label="ê²°ê³¼")
        
        convert_button.click(fn=convert_and_save, inputs=[model_id, output_dir, push_to_hub, quant_type], outputs=output)
    with gr.Tab("ì„¤ì •"):
        gr.Markdown("### ì‚¬ìš©ì ì§€ì • ëª¨ë¸ ê²½ë¡œ ì„¤ì •")
        custom_path_text = gr.Textbox(
            label="ì‚¬ìš©ì ì§€ì • ëª¨ë¸ ê²½ë¡œ",
            placeholder="./models/custom-model",
        )
        apply_custom_path_btn = gr.Button("ê²½ë¡œ ì ìš©")

        # custom_path_text -> custom_model_path_state ì €ì¥
        def update_custom_path(path):
            return path

        apply_custom_path_btn.click(
            fn=update_custom_path,
            inputs=[custom_path_text],
            outputs=[custom_model_path_state]
        )
        gr.Markdown("### ì±„íŒ… ê¸°ë¡ ì €ì¥")
        save_button = gr.Button("ì±„íŒ… ê¸°ë¡ ì €ì¥", variant="secondary")
        save_info = gr.Textbox(label="ì €ì¥ ê²°ê³¼", interactive=False)
        
        save_csv_button = gr.Button("ì±„íŒ… ê¸°ë¡ CSV ì €ì¥", variant="secondary")
        save_csv_info = gr.Textbox(label="CSV ì €ì¥ ê²°ê³¼", interactive=False)
        
        save_db_button = gr.Button("ì±„íŒ… ê¸°ë¡ DB ì €ì¥", variant="secondary")
        save_db_info = gr.Textbox(label="DB ì €ì¥ ê²°ê³¼", interactive=False)

        def save_chat_button_click_csv(history):
            if not history:
                return "ì±„íŒ… ì´ë ¥ì´ ì—†ìŠµë‹ˆë‹¤."
            saved_path = save_chat_history_csv(history)
            if saved_path is None:
                return "âŒ ì±„íŒ… ê¸°ë¡ CSV ì €ì¥ ì‹¤íŒ¨"
            else:
                return f"âœ… ì±„íŒ… ê¸°ë¡ CSVê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {saved_path}"
            
        def save_chat_button_click_db(history):
            if not history:
                return "ì±„íŒ… ì´ë ¥ì´ ì—†ìŠµë‹ˆë‹¤."
            ok = save_chat_history_db(history, session_id="demo_session")
            if ok:
                return f"âœ… DBì— ì±„íŒ… ê¸°ë¡ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤ (session_id=demo_session)"
            else:
                return "âŒ DB ì €ì¥ ì‹¤íŒ¨"

        save_csv_button.click(
            fn=save_chat_button_click_csv,
            inputs=[history_state],
            outputs=save_csv_info
        )

        # save_buttonì´ í´ë¦­ë˜ë©´ save_chat_button_click ì‹¤í–‰
        save_button.click(
            fn=save_chat_button_click,
            inputs=[history_state],
            outputs=save_info
        )
        
        save_db_button.click(
            fn=save_chat_button_click_db,
            inputs=[history_state],
            outputs=save_db_info
        )
        
        gr.Markdown('### ì±„íŒ… íˆìŠ¤í† ë¦¬ ì¬ë¡œë“œ')
        
        upload_json = gr.File(label="ëŒ€í™” JSON ì—…ë¡œë“œ", file_types=[".json"])
        load_info = gr.Textbox(label="ë¡œë”© ê²°ê³¼", interactive=False)
        
        def load_chat_from_json(json_file):
            """
            ì—…ë¡œë“œëœ JSON íŒŒì¼ì„ íŒŒì‹±í•˜ì—¬ history_stateì— ì£¼ì…
            """
            if not json_file:
                return [], "íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."
            try:
                with open(json_file.name, "r", encoding="utf-8") as f:
                    data = json.load(f)
                if not isinstance(data, list):
                    return [], "JSON êµ¬ì¡°ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. (list í˜•íƒœê°€ ì•„ë‹˜)"
                # dataë¥¼ ê·¸ëŒ€ë¡œ history_stateë¡œ ë°˜í™˜
                return data, "âœ… ëŒ€í™”ê°€ ë¡œë”©ë˜ì—ˆìŠµë‹ˆë‹¤."
            except Exception as e:
                logger.error(f"JSON ë¡œë“œ ì˜¤ë¥˜: {e}")
                return [], f"âŒ ë¡œë”© ì‹¤íŒ¨: {e}"

        upload_json.change(
            fn=load_chat_from_json,
            inputs=[upload_json],
            outputs=[history_state, load_info]
        )
        gr.Markdown("### ì„¸ì…˜ ê´€ë¦¬")
        with gr.Row():
            refresh_sessions_btn = gr.Button("ì„¸ì…˜ ëª©ë¡ ê°±ì‹ ")
            existing_sessions_dropdown = gr.Dropdown(
                label="ê¸°ì¡´ ì„¸ì…˜ ëª©ë¡",
                choices=[],  # ì´ˆê¸°ì—ëŠ” ë¹„ì–´ ìˆë‹¤ê°€, ë²„íŠ¼ í´ë¦­ ì‹œ ê°±ì‹ 
                value=None,
                interactive=True
            )
        
        with gr.Row():
            create_new_session_btn = gr.Button("ìƒˆ ì„¸ì…˜ ìƒì„±")
            apply_session_btn = gr.Button("ì„¸ì…˜ ì ìš©")
        
        session_manage_info = gr.Textbox(
            label="ì„¸ì…˜ ê´€ë¦¬ ê²°ê³¼",
            interactive=False
        )
        
        def refresh_sessions():
            """
            ì„¸ì…˜ ëª©ë¡ ê°±ì‹ : DBì—ì„œ ì„¸ì…˜ IDë“¤ì„ ë¶ˆëŸ¬ì™€ì„œ Dropdownì— ì—…ë°ì´íŠ¸
            """
            sessions = get_existing_sessions()
            if not sessions:
                return gr.update(choices=[], value=None), "DBì— ì„¸ì…˜ì´ ì—†ìŠµë‹ˆë‹¤."
            return gr.update(choices=sessions, value=sessions[0]), "ì„¸ì…˜ ëª©ë¡ì„ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤."
        
        def create_new_session():
            """
            ìƒˆ ì„¸ì…˜ IDë¥¼ ë§Œë“  ë’¤, session_id_stateì— ë°˜ì˜
            """
            new_sid = secrets.token_hex(8)
            # ì‹¤ì œë¡œëŠ” DBì— ë„£ì„ í•„ìš”ëŠ” ì—†ìœ¼ë©°, ì±„íŒ… ìµœì´ˆ ì €ì¥ ì‹œ ìë™ìœ¼ë¡œ ë“¤ì–´ê°ˆ ê²ƒ
            return new_sid, f"ìƒˆ ì„¸ì…˜ ìƒì„±: {new_sid}"

        def apply_session(chosen_sid):
            """
            Dropdownì—ì„œ ì„ íƒëœ ì„¸ì…˜ IDë¡œ, DBì—ì„œ historyë¥¼ ë¶ˆëŸ¬ì˜¤ê³ , session_id_stateë¥¼ ê°±ì‹ 
            """
            if not chosen_sid:
                return [], None, "ì„¸ì…˜ IDë¥¼ ì„ íƒí•˜ì„¸ìš”."
            loaded_history = load_chat_from_db(chosen_sid)
            # history_stateì— ë°˜ì˜í•˜ê³ , session_id_stateë„ ì—…ë°ì´íŠ¸
            return loaded_history, chosen_sid, f"ì„¸ì…˜ {chosen_sid}ì´ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤."
        
        # ë²„íŠ¼ ì´ë²¤íŠ¸ ì—°ê²°
        refresh_sessions_btn.click(
            fn=refresh_sessions,
            inputs=[],
            outputs=[existing_sessions_dropdown, session_manage_info]
        )
        
        create_new_session_btn.click(
            fn=create_new_session,
            inputs=[],
            outputs=[session_id_state, session_manage_info]
        )
        
        apply_session_btn.click(
            fn=apply_session,
            inputs=[existing_sessions_dropdown],
            outputs=[history_state, session_id_state, session_manage_info]
        ).then(
            fn=filter_messages_for_chatbot, # (2) ë¶ˆëŸ¬ì˜¨ historyë¥¼ Chatbot í˜•ì‹ìœ¼ë¡œ í•„í„°ë§
            inputs=[history_state],
            outputs=chatbot                 # (3) Chatbot ì—…ë°ì´íŠ¸
        )

demo.launch(debug=True, inbrowser=True, server_port=7861, width=500)