{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bf3a1b9ff6746d4b1cd17dd14401faf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델이 성공적으로 8비트로 변환되어 './models/transformers/genai-archive__glm-4-9b-chat-hf-4bit'에 저장되었습니다.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model_converter import convert_model_to_int4\n",
    "\n",
    "convert_model_to_int4(\"./models/transformers/THUDM__glm-4-9b-chat-hf\", output_dir=\"./models/transformers/genai-archive__glm-4-9b-chat-hf-4bit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RepoUrl('https://huggingface.co/bean980310/glm-4-9b-chat-hf-int4', endpoint='https://huggingface.co', repo_type='model', repo_id='bean980310/glm-4-9b-chat-hf-int4')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import create_repo, upload_folder\n",
    "\n",
    "create_repo(repo_id=\"bean980310/glm-4-9b-chat-hf-int4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.quanto import freeze, qint8, quantize\n",
    "from diffusers import StableDiffusionXLPipeline\n",
    "from safetensors import serialize_file\n",
    "import torch\n",
    "\n",
    "pipeline = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"cagliostrolab/animagine-xl-3.1\", \n",
    "    torch_dtype=torch.bfloat16,\n",
    ").to(\"cpu\")\n",
    "quantize(pipeline.unet, weights=qint8)\n",
    "freeze(pipeline.unet)\n",
    "quantize(pipeline.text_encoder, weights=qint8)\n",
    "freeze(pipeline.text_encoder)\n",
    "quantize(pipeline.text_encoder_2, weights=qint8)\n",
    "freeze(pipeline.text_encoder_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.save_pretrained(\"diffusion_models/animagine-xl-3.1-int8\", safe_serialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.quanto import QuantizedDiffusersModel\n",
    "from diffusers import StableDiffusionXLPipeline\n",
    "from safetensors import serialize_file\n",
    "import torch\n",
    "\n",
    "class QuantizedStableDiffusionXLPipeline(QuantizedDiffusersModel):\n",
    "    base_class = StableDiffusionXLPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# safetensors 파일에서 파이프라인 로드\n",
    "pipeline_loaded = QuantizedStableDiffusionXLPipeline.from_pretrained(\n",
    "    \"diffusion_models/animagine-xl-3.1-int8\",\n",
    "    torch_dtype=torch.float16,\n",
    ").to(\"mps\")  # 또는 \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import load_file\n",
    "unet_safetensors_path='diffusion_models/animagine-xl-3.1-int8/unet/diffusion_pytorch_model.safetensors'\n",
    "\n",
    "unet_state_dict_loaded = load_file(unet_safetensors_path)\n",
    "pipeline_loaded.unet.load_state_dict(unet_state_dict_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prefixed_unet_state_dict = {f\"unet.{k}\": v.cpu() for k, v in unet_state_dict.items()}\n",
    "prefixed_text_encoder_state_dict = {f\"text_encoder.{k}\": v.cpu() for k, v in text_encoder_state_dict.items()}\n",
    "prefixed_text_encoder_2_state_dict = {f\"text_encoder_2.{k}\": v.cpu() for k, v in text_encoder_state_dict.items()}\n",
    "\n",
    "# 전체 state_dict 결합\n",
    "combined_state_dict = {**prefixed_unet_state_dict, **prefixed_text_encoder_state_dict}\n",
    "\n",
    "combined_state_dict_2 = {**combined_state_dict, **prefixed_text_encoder_2_state_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import save_file\n",
    "\n",
    "# safetensors 파일로 저장\n",
    "safetensors_path = \"diffusion_models/animagine-xl-3.1-int8.safetensors\"\n",
    "save_file(combined_state_dict_2, safetensors_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
